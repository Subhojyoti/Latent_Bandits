Our work lies at the intersection of several exciting areas, which we survey below.

\textbf{Latent Bandits:} The existing algorithms in latent bandit literature can be broadly classified into two groups: the online matrix completion algorithms and the online contextual bandit  algorithms. The \textit{online matrix completion algorithms} try to reconstruct the reward matrix $M$ from a noisy realization combining different approaches of online learning algorithms and matrix factorization algorithms. The NMF-Bandit algorithm in \citet{sen2016contextual} is an online matrix completion algorithm which is an $\epsilon$-greedy algorithm that tries to reconstruct the matrix $M$ through non-negative matrix factorization. Note, that this approach requires that all the matrices satisfy a weak statistical Restricted Isometric Property, which is not always feasible in real life applications. Another approach is that of \citet{gopalan2016low} where the authors come up with an algorithm which uses the Robust Tensor Power (RTP) method of \citet{anandkumar2014tensor} to reconstruct the matrix $M$, and then use the OFUL procedure of \citet{abbasi2011improved} to behave greedily over the reconstructed matrix. But the RTP is a costly operation whereas the learner needs to construct a matrix of order $L\times L$ and $L\times L \times L$ to calculate the second and third order tensors for the reconstruction.  A more simpler setting has also been studied in \citet{maillard2014latent} where all the users tend to come from only one class and hence this approach is also not quite realistic. 

	The second type of algorithms are the \textit{online contextual bandit algorithms} where for each user $i\in[K]$ a separate instance of a base-bandit algorithm is implemented to find the best item for the user. These base-bandits can be randomized algorithms suited for the adversarial setting like EXP3 \citep{auer2002nonstochastic} or UCB type algorithms suited for the stochastic setting llike UCB1 \citep{auer2002finite}, MOSS \citep{audibert2009minimax}, OCUCB \citep{lattimore2015optimally}, KL-UCB \citep{cappe2013kullback}, \citep{garivier2011kl} or Bayesian algorithms like Thompson Sampling \citep{thompson1933likelihood}, \citep{thompson1935theory}, \citep{agrawal2012analysis}.

%can be used for this purpose.
%which are a set of frequentist indexed based algorithms
% Several powerful variation of the stochastic or non-stochastic multi-armed bandit algorithms can be used for this purpose.
%method with $\epsilon$ probability or with $1-\epsilon$ it behaves greedily over the already reconstructed matrix $\hat{M}$.

\textbf{Ranked Bandits:} Bandits have been used to rank items for online recommendations where the goal is is to present a list of $d$ items out of $L$ that maximizes the satisfaction of the user. A popular approach is to model each of the $d$ rank positions as a Multi Armed Bandit (MAB) problem and use a base-bandit algorithm to solve it. This was first proposed in \citet{radlinski2008learning} which  showed that query abandonment by user can also be successfully used to learn rankings. Later works on ranking such as \citet{slivkins2010ranked} and \citet{slivkins2013ranked} uses additional assumptions to handle  exponentially large number of items such that items and user models lie within a metric space and satisfy Lipschitz condition. 

\textbf{Ranking in Click Models:} Several algorithms have been proposed to solve the ranking problem in specific click models. Popular click models that have been studied extensively are Document Click Model (DCM), Position Based Click Model (PBM) and Cascade Click Model (CBM). For a survey of existing click models a reader may look into \citet{chuklin2015click}. While \citet{katariya2017bernoulli}, \citet{katariya2016stochastic} works in PBM, \citet{zoghi2017online} works in both PBM and CBM. Finally, \citet{kveton2017stochastic} can be viewed as a generalization of rank-1 bandits of \citet{katariya2016stochastic} to a higher rank. Note, that the theoretical guarantees of these algorithms does not hold beyond the specific click models.


\textbf{Online Sub-modular maximization:} Maximization of submodular functions has wide applications in machine learning, artificial
intelligence and in recommender systems \citep{nemhauser1978analysis}, \citep{krause2014submodular}. A submodular function $f : 2^V \rightarrow \mathbb{R}$ for a finite ground set $V$ is a set function that assign each
subset $S \subseteq V$ a value $f(S)$. We define the gain of the function $f$ as $G_f(e|S) = f(S \cup \lbrace e\rbrace) - f(S)$ where the element $\lbrace e\rbrace \in V\setminus S$ and $S \subseteq V$. Also, $f$ satisfies the following two criteria:-
\begin{enumerate}
\item Monotonicity: A set function $f : 2^V \rightarrow \mathbb{R}$ is monotone if for every $A \subseteq B \subseteq V, f(A) \leq f(B)$.

\item Submodularity: A set function $f : 2^V \rightarrow \mathbb{R}$ is submodular if for every $A \subseteq B \subseteq V$ and $\lbrace e\rbrace \in V \setminus B$ it holds that $G_f(e | A) \geq G_f(e | B)$.
\end{enumerate}

Intuitively, a submodular function states that after performing a set $A$ of actions, the marginal gain of another action $e$ does not increase the gain for performing other actions in $B \setminus A$. Online submodular function maximization has been studied in \citet{streeter2009online} where the authors propose a general algorithm whereas  \citet{radlinski2008learning} can be considered as special case of it when the payoff is only between $\lbrace 0, 1\rbrace$. Also, in the contextual feature based setup online  submodular maximization has been studied by  \citet{yue2011linear}. An interesting property of submodular function is that a greedy algorithm using it is guaranteed to perform atleast $\left( 1 - \frac{1}{e}\right)$ of the optimal algorithm and this factor $\left( 1 - \frac{1}{e}\right)$ is not improvable by any polynomial time algorithm \citep{nemhauser1978analysis}.


%The existing algorithms in latent bandit literature can be broadly classified into two groups: the online matrix completion algorithms and the online contextual bandit  algorithms. The online matrix completion algorithms combines two different approaches of online learning algorithms and matrix factorization algorithms. In this approach first an online learning algorithm samples entries from a noisy realization of the reward matrix $M$ and constructs a partially completed version of this  reward matrix, denoted by $\hat{M}$. Then a matrix factorization algorithm decomposes this partially observed reward matrix of size $K\times L$ into two smaller sized matrices of dimension $\hat{ U} = K\times d$ and $\hat{V} = L\times d$ and tries to reconstruct the original reward matrix such that $M\approx \hat{M} = \hat{U}\hat{V}^{\intercal}$. Note, that these type of approach almost always have access to the rank of the matrix $M$ i.e. $d$. The NMF-Bandit algorithm in \citet{sen2016contextual} is an online matrix completion algorithm which is an $\epsilon$-greedy algorithm that tries to reconstruct the matrix $M$ through non-negative matrix factorization method with $\epsilon$ probability or with $1-\epsilon$ it behaves greedily over the already reconstructed matrix $\hat{M}$. Note, that this approach uses several assumption on the nature of the matrix $M,U$ and $V$ such that $U$ and $V$ satisfies a weak statistical Restricted Isometric Property, which is not always feasible in real life datasets. Another approach is that of \citet{gopalan2016low} where the authors come up with an algorithm which uses the Robust Tensor Power (RTP) method of \citet{anandkumar2014tensor} to construct the matrix $\hat{M}\approx M$, and then use the OFUL procedure of \citet{abbasi2011improved} to behave greedily over the reconstructed matrix $\hat{M}$. But the RTP is a costly operation whereas the learner needs to construct a matrix of order $L\times L$ and $L\times L \times L$ to calculate the second and third order tensors in order to build $\hat{M}$. Moreover, to apply RTP several assumption regarding eigenvalues needs to be satisfied which is again not feasible for real-life scenarios.  A much more simpler setting has also been studied in \citet{maillard2014latent} where the users tend to come from only one class and hence this approach is also not quite realistic.

%The second type of algorithms are the online contextual bandit algorithms where for each user a separate instance of a bandit algorithm is implemented to find the best item for the user. Several powerful variation of the stochastic multi-armed bandit algorithms can be used for this purpose. These can be randomized algorithms suited for the adversarial settings like EXP3  \citet{auer2002nonstochastic} or UCB type algorithms suited for the stochastic setting llike UCB1 \citep{auer2002finite}, MOSS \citep{audibert2009minimax}, OCUCB \citep{lattimore2015optimally}, KL-UCB \citep{cappe2013kullback}, \citep{garivier2011kl} which are a set of frequentist indexed based algorithms or the Bayesian algorithms like TS \citep{thompson1933likelihood}, \citep{thompson1935theory}, \citep{agrawal2012analysis} can be used for this purpose.

%In \citet{maillard2014latent} the authors propose the Latent Bandit model where there are two sets: 1) set of arms denoted by $\A$ and 2) set of types denoted by $\B$ which contains the latent information regarding the arms. The latent information for the arms are modeled such that the set $\B$ is assumed to be partitioned into $|C|$ clusters, indexed by $\B_1, \B_2, \ldots, \B_C \in \C$ such that the distribution $v_{a,b}, a\in\A, b\in\B_c$ across each cluster is same.  Note, that the identity of the cluster is unknown to the learner. At every timestep $t$, nature selects a type $b_t\in\B_c$ and then the learner selects an arm $a_t\in\A$ and observes a reward $r_{t}(a,b)$ from the distribution $v_{a,b}$.
%	
%	Another way to look at this problem is to imagine a matrix of dimension $|A|\times |B|$ where again the rows in $\B$ can be partitioned into $|C|$ clusters, such that the distribution across each of this clusters are same. Now, at every timestep $t$ one of this row is revealed to the learner and it chooses one column such that the $v_{a,b}$ is one of the $\lbrace v_{a,c}\rbrace_{c\in\C}$ and the reward for that arm and the user is revealed to the learner.
%	
%	This is actually a much simpler approach than the setting we considered  because note that the distributions across each of the clusters $\lbrace v_{a,c}\rbrace_{c\in\C}$ are identical and estimating one cluster distribution will reveal all the information of the users in each cluster.