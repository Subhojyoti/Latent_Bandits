Our contributions are mainly three fold. First, we formulate the Latent Ranked Bandit problem as an online learning problem on a class of low-rank non-negative matrices that can be solved efficiently without estimating the latent factors that generate the matrices. We borrow ideas from the ranking literature to solve this problem efficiently and term this setting as personalized ranking setting. Secondly, we propose the Latent Ranked Bandit Algorithm (LRB) for this personalized ranking setting. LRB has two components, the column MABs  and the row MABs that work simultaneously to find a diverse list of items that will contain the best item at rank $1$ position with high probability. The column MABs leverage the low rank structure of the user-preference matrix $M$ and quickly finds out the $d$ best items. Simultaneously the row MABs permute the suggested list of $d$ items to find the best item at rank $1$ position for the individual users. Finally, we show that an instance of LRB which uses exponential weighting algorithm EXP3 as column MABs and Weighted Majority Algorithm (WMA) as row MABs suffers a regret of atmost $O\left(\sqrt{â€¢}\right)$. On diverse experimental settings we test our proposed algorithm and show improved performance even when our modeling assumptions does not hold.

	The rest of the paper is organized as follows. We survey the existing literature in Section \ref{related}. Then we state our setting, assumptions and notations in Section \ref{probdef}. We propose the algorithm LRB in Section \ref{algo} and in Section \ref{analysis} we analyze LRB. Finally, we show experiments in Section \ref{expt} and we conclude in Section \ref{conclusions}. The proof is contained in the Appendix \ref{sec:proof}.