Our contributions are mainly three fold. First, we formulate the latent ranked bandit problem as an online learning problem on a class of low-rank non-negative matrices that can be solved efficiently without estimating the latent factors that generate the matrices. We borrow ideas from the ranking literature to solve this problem efficiently and term this setting as personalized ranking setting. This is because an efficient algorithm can find a global ranking of best items and then permute that list to find a personalized ranking for individual users, sorted by their preference from high to low. Secondly, we propose the Latent Ranker Algorithm (LRA) for this personalized ranking setting. LRA has two components, the column MABs  and the row MABs that work simultaneously to find a diverse list of items that will contain the best item at rank $1$ position with high probability. The column MABs leverage the low rank structure of the user-preference matrix $M$ and quickly finds out the $d$ best items. Simultaneously the row MABs permute the suggested list of $d$ items to find the best item at rank $1$ position for the individual users. Finally, we show that an instance of LRB which uses exponential weighting algorithm EXP3 as column MABs and weighted majority algorithm (WMA) as row MABs suffer a regret of atmost $O\left(d^2\sqrt{L n} + K \log n\right)$. On diverse experimental settings we test our proposed algorithm and show improved performance even when our modeling assumptions does not hold.

	The rest of the paper is organized as follows. Then we state our setting, assumptions and notations in Section \ref{probdef}. We propose the algorithm LRB in Section \ref{algo} and in Section \ref{analysis} we analyze LRB. Finally, we show experiments in Section \ref{expt} and give a brief survey of the existing literature in Section \ref{related}. We conclude in Section \ref{conclusions} while the proof is contained in the Appendix \ref{sec:proof}.