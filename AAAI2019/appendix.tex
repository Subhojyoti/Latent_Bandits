%!TEX root = paper.tex

\clearpage
\onecolumn
\appendix

\section{Proof}
\label{sec:proof}

The reward for recommending $d$ columns $J$ to user $i$ is
\begin{align*}
  r_t(i, J) =
  \max \, \{\mu(k) \, r_t(i, J(k)): k \in [d]\}
\end{align*}
for weights $\mu(1) \geq \dots \geq \mu(d) > 0$. We also define the corresponding unweighted reward as
\begin{align*}
  \tilde{r}_t(i, J) =
  \max \, \{r_t(i, J(k)): k \in [\text{length}(J)]\}\,.
\end{align*}
Let $J_\ast$ be the indices of hott topics and $\pi_{\ast, i}$ be their highest-reward permutation for user $i$. Let $J_t$ be our recommended columns at time $t$ and $\pi_{t, i}$ be their permutation for user $i$, which is computed by some later-defined row algorithm. Then the expected $n$-step regret of our learning agent is
\begin{align*}
  R(n) =
  \sum_{t = 1}^n \E\left[r_t(i_t, \pi_{\ast, i_t}(J_\ast)) - r_t(i_t, \pi_{t, i_t}(J_t))\right]\,.
\end{align*}
To decompose the regret into its row and column components, we introduce event $1\{J_t \neq J_\ast\}$,
\begin{align*}
  R(n)
  & = \sum_{t = 1}^n \E\left[1\{J_t \neq J_\ast\} (r_t(i_t, \pi_{\ast, i_t}(J_\ast)) - r_t(i_t, \pi_{t, i_t}(J_t)))\right] +
  \sum_{t = 1}^n \E\left[1\{J_t = J_\ast\} (r_t(i_t, \pi_{\ast, i_t}(J_\ast)) - r_t(i_t, \pi_{t, i_t}(J_\ast)))\right] \\
  & \leq \sum_{t = 1}^n \E\left[1\{J_t \neq J_\ast\}\right] +
  \sum_{i = 1}^K \E\left[\sum_{t = 1}^n 1\{i_t = i, J_t = J_\ast\} (r_t(i, \pi_{\ast, i}(J_\ast)) - r_t(i, \pi_{t, i}(J_\ast)))\right]\,,
\end{align*}
where the inequality follows from the fact that the maximum instantaneous regret is $1$.

%The first term in the regret decomposition can be bounded as follows. 
%Let
%\begin{align*}
%  g_{t, i}(J, j) =
%  \tilde{r}_t(i, (J, j)) - \tilde{r}_t(i, J)
%\end{align*}
%be the gain in reward from adding item $j$ to items $J$, for user $i$ at time $t$. Let $J_\ast$ be defined greedily as
%\begin{align*}
%  J_\ast(k) =
%  \argmax_{j \in [L] \setminus J_\ast(: k - 1)} \sum_{t = 1}^n \sum_{i = 1}^K g_{t, i}(J_\ast(: k - 1), j)
%\end{align*}
%for $k \in [d]$. From the design of our column learning algorithm, for any sequence of users $i_1, \dots, i_n$ and $k \in [d]$,
%\begin{align}
%  \sum_{t = 1}^n \E\left[g_{t, i_t}(J_t(: k - 1), J_\ast(k)) - g_{t, i_t}(J_t(: k - 1), J_t(k))\right] \leq
%  \sqrt{L n}\,.
%  \label{eq:single column regret}
%\end{align}
%Now note that
%\begin{align*}
%  & \sum_{t = 1}^n \E\left[\tilde{r}_t(i_t, J_\ast) - \tilde{r}_t(i_t, J_t)\right] = \\
%  & \quad \sum_{t = 1}^n \sum_{k = 1}^d \E\left[g_{t, i_t}(J_\ast(: k - 1), J_\ast(k)) - g_{t, i_t}(J_t(: k - 1), J_t(k))\right] = \\
%  & \quad \sum_{t = 1}^n \sum_{k = 1}^d \E\left[g_{t, i_t}(J_\ast(: k - 1), J_\ast(k)) - g_{t, i_t}(J_t(: k - 1), J_\ast(k)) +
%  g_{t, i_t}(J_t(: k - 1), J_\ast(k)) g_{t, i_t}(J_t(: k - 1), J_t(k))\right] \leq \\
%  & \quad \sum_{t = 1}^n \sum_{k = 1}^d \E\left[g_{t, i_t}(J_\ast(: k - 1), J_\ast(k)) - g_{t, i_t}(J_t(: k - 1), J_\ast(k))\right] +
%  d \sqrt{L n} \leq \\
%  & \quad d \sqrt{L n}\,,
%\end{align*}
%where the first inequality follows from \eqref{eq:single column regret}, and the second inequality is from the greedy selection of $J_\ast(: k - 1)$ and $J_\ast(k)$. \todob{We still need to prove the latter claim.}
 Let
\begin{align*}
  \Delta =
  \min_{t \in [n]} \min_{J:\, J \neq J_\ast} \E\left[\tilde{r}_t(i, J_\ast)\right] - \E\left[\tilde{r}_t(i, J)\right]
\end{align*}
be the minimum gap between the optimal and best suboptimal columns, averaged over users. Then by Lemma~\ref{lem:keylem}, we have
\begin{align*}
  \sum_{t = 1}^n \E\left[1\{J_t \neq J_\ast\}\right] \leq
  \frac{d \sqrt{L n}}{\Delta}\,.
\end{align*}
The second term in the regret decomposition can be bounded as
\begin{align*}
  \sum_{i = 1}^K \E\left[\sum_{t = 1}^n 1\{i_t = i, J_t = J_\ast\} (r_t(i, \pi_{\ast, i}(J_\ast)) - r_t(i, \pi_{t, i}(J_\ast)))\right] \leq
  \sum_{i = 1}^K R_i(n)\,,
\end{align*}
where $R_i(n)$ is the expected $n$-step regret of the row algorithm in row $i$, conditioned on the event that the column algorithm chooses $J_\ast$. One suitable row algorithm is the weighted majority algorithm, which learns the optimal permutation for each $J$. Conditioned on $J$, this is a full-information setting with $d!$ arms. Therefore, $R_i(n) = O(\log n + \log d!) = O(\log n + d \log d)$. Now we chain all above inequalities and get that
\begin{align*}
  R(n) = O\left(\frac{d \sqrt{L n}}{\Delta} + K \log n + K d \log d\right)\,.
\end{align*}

\todob{Before the we submit the paper, I will add discussion on $\Delta$ and also prove a bound where we integrate it out.}

For a vector $J$, we will use the notation $J[:k]$ to denote the vector $(J(1),...,J(k)).$
\begin{lemma}
\label{lem:keylem}
For any $k \in [d]$,
$$ \sum_t \E \tilde{r}_t(i, J_t[:k]) \geq  \E \sum_t \tilde{r}_t(i, J^*[:k]) - O(k\sqrt{nL}).$$ 
\end{lemma}
\begin{proof}
We will show this by induction. Note that there are $d$ column MABs. The base case when $k=1$ follows because of the guarantees of MAB$_1(n)$.
Let $J^* = (j_1^*,j_2^*,...,j_d^*).$   We will now assume that the result is true for $k-1$ for some $k>1.$
We have 
\begin{align}
& \E  \sum_t \tilde{r}_t(i, J_t(:k))   \\
&\geq \max_{j_k} \E \sum_t   \max \{ \tilde{r}_t(i, J_t(:k-1)), r_t(i_t,j_k) \}  - O\left(\sqrt{nL}\right) \\
&\geq \max_{j_k} \E \sum_t   \max \{  \tilde{r}_t(i, J^*(:k-1)), r_t(i_t,j_k) \}   - O\left( \sqrt{nL} \right) - O\left( (k-1)\sqrt{nL} \right)   \\
\label{eq:result}
& =  \E \sum_t  \tilde{r}_t(i, J^*(:k))  - O\left( k\sqrt{nL} \right).
\end{align}
%\todob{The last term in the first line misses $\max$. But a more important question is why do we need that term at all. The term is constant throughout the derivation.}

The last equality follows from Lemma~\ref{thm:hott}.  \todoan{Need a (simple) lemma about hott topics structure}
The first inequality is from the guarantees of MAB$_k(n)$. \todob{State the regret bound of Exp3 in a lemma.} The crucial step is  the second inequality. It says that we can replace $J(:k-1)$ with $J^*(:k-1)$ by just losing another additive $O\left( (k-1) \sqrt{nL} \right)$ term. This follows from induction hypothesis and Lemma~\ref{lem:keyinequality}. We note that from Equation~\ref{eq:result}, we have
$$ \max  \sum_t \tilde{r}_t(i, J_t[:k])  \geq  \E \sum_t \max  \sum_t \tilde{r}_t(i, J^*[:k])  - O\left( k \sqrt{nL} \right),$$
which concludes the proof.
\end{proof}


\begin{lemma}
\label{lem:keyinequality}
Suppose
 $$ \E \sum_t \tilde{r}_t(i_t, J_t(:k-1) \geq  \E \sum_t \tilde{r}_t(i_t, J^*(:k-1))  - C$$
 %\todob{Define $C$. Why not just $C$?}
  and let $j_k \in [L].$ Then,
$$ \E \sum_t \max \{ \tilde{r}_t(i_t, J_t(:k-1)), r_t(i_t,j_k) \} \geq \E \sum_t  \max \{ \tilde{r}_t(i_t, J^*(:k-1)), r_t(i_t,j_k) \} - O\left((k-1) \sqrt{nL} \right).$$
%\myworries{Both Brano and I have confused ourselves over variants of this lemma. Definitely helpful for everyone to verify if the proof is correct.}
\end{lemma}

\begin{proof}
 Let $T_1 = \{ t \, | \tilde r_t(i_t,J^*(:k-1)) < r_t(i_t,j_k) \}$ and $T_2 = [n] \backslash T_1 .$ We then have 
\begin{align*}
  & \E \sum_t \max \{ \tilde{r}_t(i_t,J_t(:k-1)),r_t(i_t,j_k) \} \\
  & \quad = \E\sum_{t \in T_1} \max \{ \tilde{r}_t(i_t,J_t(:k-1)),r_t(i_t,j_k) \} + \E\sum_{t \in T_2} \max \{ \tilde{r}_t(i_t,J_t(:k-1)),r_t(i_t,j_k) \}  \\
  & \quad \geq \sum_{t \in T_1} \max \{ \tilde{r}_t(i_t,J^*(:k-1)),r_t(i_t,j_k) \} + \E\sum_{t \in T_2} \max \{ \tilde{r}_t(i_t,J_t(:k-1)),r_t(i_t,j_k) \} \\
  & \quad \geq \sum_{t \in T_1} \max \{ \tilde{r}_t(i_t,J^*(:k-1)),r_t(i_t,j_k) \}) + \E\sum_{t \in T_2} \tilde{r}_t(i_t,J_t(:k-1)) \\
  & \quad \geq \sum_{t \in T_1}\max \{ \tilde{r}_t(i_t,J^*(:k-1)),r_t(i_t,j_k) \} + \sum_{t \in T_2} \tilde{r}_t(i_t,J^*(:k-1)) - C \\
  & \quad = \sum_{t \in T_1} \max \{ \tilde{r}_t(i_t,J^*(:k-1)),r_t(i_t,j_k) \} + \sum_{t \in T_2} \max \{ \tilde{r}_t(i_t,J^*(:k-1)),r_t(i_t,j_k) \} - C \\
  & \quad = \sum_{t \in [n]}  \max \{ \tilde{r}_t(i_t,J^*(:k-1)),r_t(i_t,j_k) \} - C.
\end{align*}
%\todob{$(j_1^*,j_2)$ should be $(J^*[1:k-1],j_k)$. $j_2$ should be $j_k$. The same below.}
The first inequality is easy because $\max R_t(i_t,(J^*[1:k-1],j_k) = R_t(i_t,j_k)$ for $t \in T_1$. Second inequality is trivial. Third inequality follows from the assumption. The next equality holds because of the definition of $T_2$.  
\end{proof}


%\newcommand{\transpose}{^\mathsf{\scriptscriptstyle T}}
%
%\section{Proof}
%\label{sec:proof}
%
%Our learning agent operates in the following setting. Let $i_1, \dots, i_n$ be a fixed sequences of users in $n$ steps, which is unknown to the agent. Let $r_t = U D_t V\transpose$ be the reward matrix at time $t$, where $U$ is a non-negative matrix, $D_t$ is a non-negative diagonal matrix that may change with $t$, and $V$ is a non-negative hott-topics matrix. We assume that $r_t \in [0, 1]^{K \times L}$ at all times $t \in [n]$. The only randomness in our problem is due to the learning agent.
%
%The reward for recommending $d$ columns $J$ to user $i$ is
%\begin{align*}
%  r_t(i, J) =
%  \max \, \{\mu(k) \, r_t(i, J(k)): k \in [d]\}
%\end{align*}
%for weights $\mu(1) \geq \dots \geq \mu(d) > 0$. We also define the corresponding unweighted reward as
%\begin{align*}
%  \tilde{r}_t(i, J) =
%  \max \, \{r_t(i, J(k)): k \in [d]\}\,.
%\end{align*}
%Let $J_\ast$ be the indices of hott topics and $\pi_{\ast, i}$ be their highest-reward permutation for user $i$. Let $J_t$ be our recommended columns at time $t$ and $\pi_{t, i}$ be their permutation for user $i$, which is computed by some later-defined row algorithm. The expected $n$-step regret, where the only randomness is due to the learning agent, is
%\begin{align*}
%  R(n) =
%  \E\left[\sum_{t = 1}^n r_t(i_t, \pi_{\ast, i_t}(J_\ast))\right] - \E\left[\sum_{t = 1}^n r_t(i_t, \pi_{t, i_t}(J_t))\right]\,.
%\end{align*}
%The regret of the column learning algorithm in $n_0$ steps is bounded as
%\begin{align*}
%  \E\left[\sum_{t = 1}^{n_0} \tilde{r}_t(i_t, J_\ast)\right] - \E\left[\sum_{t = 1}^{n_0} \tilde{r}_t(i_t, J_t)\right] \leq
%  d \sqrt{L n_0}
%\end{align*}
%for any $n_0$, based on a similar analysis to ranked bandits. Let
%\begin{align*}
%  \Delta = \min_{i \in [K], t \in [n]} \left(\tilde{r}_t(i, J_\ast) - \max_{J:\, J \neq J_\ast} \tilde{r}_t(i, J)\right)
%\end{align*}
%be the minimum gap. \todob{The above definition of the gap needs to be adjusted. It is zero whenever $J$ contains the optimal column for user $i$. Our algorithm is sound and learns $J^\ast$. So this is just a technicality.} Then, based on the above inequalities, the probability that the column learning algorithm chooses $J_\ast$ at any time $t \geq n_0$ is bounded from below by
%\begin{align}
%  1 - \frac{d \sqrt{L n_0}}{\Delta n_0} =
%  \frac{\Delta \sqrt{n_0} - d \sqrt{L}}{\Delta \sqrt{n_0}}
%  \label{eq:opt lower bound}
%\end{align}
%for any $\Delta \geq d \sqrt{L / n_0}$.
%
%Let $p_t$ be the probability that the column learning algorithm chooses $J_\ast$ at time $t$ and let $\pi_{t, i}(J_\ast)$ be its permutation for user $i$ at time $t$, according to our row algorithm. Then we can bound the regret from time $n_0$ as
%\begin{align*}
%  R(n)
%  & = \sum_{i = 1}^K \E\left[\sum_{t = n_0}^n 1\{i_t = i\}
%  (r_t(i, \pi_{\ast, i}(J_\ast)) - r_t(i, \pi_{t, i}(J_t))\right] \\
%  & = \sum_{i = 1}^K \E\left[\sum_{t = n_0}^n \frac{1}{p_t} 1\{i_t = i, J_t = J_\ast\}
%  (r_t(i, \pi_{\ast, i}(J_\ast)) - r_t(i, \pi_{t, i}(J_\ast))\right] \\
%  & \leq \left(1 + \frac{d \sqrt{L}}{\Delta \sqrt{n_0} - d \sqrt{L}}\right) \sum_{i = 1}^K R_i(n)\,,
%\end{align*}
%where $R_i(n)$ is the expected $n$-step regret of the row algorithm in row $i$, conditioned on the fact that the column learning algorithm chooses $J_\ast$. One suitable row algorithm would be the weighted majority algorithm, which learns the optimal permutation for each $J$. Then $R_i(n) = O(\log n + \log d!) \approx O(\log n + d \log d)$.
%
%In the first $n_0$ steps, we bound the regret trivially by $n_0$. Then the expected $n$-step regret is bounded up to log factors as
%\begin{align*}
%  R(n) \leq
%  n_0 + \left(1 + \frac{d \sqrt{L}}{\Delta \sqrt{n_0} - d \sqrt{L}}\right) K\,.
%\end{align*}
%The bound can be interpreted as follows. Choose some reasonable $n_0$ that makes the regret comparable to ranked bandits, such as $n_0 = c^2 d \sqrt{L n}$ for some $c > 0$. Then the multiplier at $K$ becomes smaller than $2$, for instance, when $\Delta c d^\frac{1}{2} L^\frac{1}{4} n^\frac{1}{4} \geq 2 d L^\frac{1}{2}$. Under the assumption that $n \geq L$, this happens when $\Delta \geq 2 \sqrt{d} / c$, which makes sense for $c > 2 \sqrt{d}$\,.
