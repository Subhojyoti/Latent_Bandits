%!TEX root = LatentBandits.tex

\clearpage
\onecolumn
\appendix

\newcommand{\transpose}{^\mathsf{\scriptscriptstyle T}}

\section{Analysis}
\label{sec:analysis}

Our learning agent operates in the following setting. Let $i_1, \dots, i_n$ be a fixed sequences of users in $n$ steps, which is unknown to the agent. Let $r_t = U D_t V\transpose$ be the reward matrix at time $t$, where $U$ is a non-negative matrix, $D_t$ is a non-negative diagonal matrix that may change with $t$, and $U$ is a non-negative hott-topics matrix. We assume that $r_t \in [0, 1]^{K \times L}$ at all times $t \in [n]$. The only randomness in our problem is due to the learning agent.

The reward for recommending $d$ columns $J$ to user $i$ is
\begin{align*}
  r_t(i, J) =
  \max \, \{\mu(k) \, r_t(i, J(k)): k \in [d]\}
\end{align*}
for weights $\mu(1) \geq \dots \geq \mu(d) > 0$. We also define the corresponding unweighted reward as
\begin{align*}
  \tilde{r}_t(i, J) =
  \max \, \{r_t(i, J(k)): k \in [d]\}\,.
\end{align*}
Let $J_\ast$ be the indices of hott topics and $\pi_{\ast, i}$ be their highest-reward permutation for user $i$. Let $J_t$ be our recommended columns at time $t$ and $\pi_{t, i}$ be their permutation for user $i$, which is computed by some later-defined row algorithm. The expected $n$-step regret, where the randomness is only in the learning agent, is
\begin{align*}
  R(n) =
  \E\left[\sum_{t = 1}^n r_t(i_t, \pi_{\ast, i_t}(J_\ast))\right] - \E\left[\sum_{t = 1}^n r_t(i_t, \pi_{t, i_t}(J_t))\right]\,.
\end{align*}
The regret of the column learning algorithm in $n_0$ steps is bounded as
\begin{align*}
  \E\left[\sum_{t = 1}^{n_0} \tilde{r}_t(i, J_\ast)\right] - \E\left[\sum_{t = 1}^{n_0} \tilde{r}_t(i, J_t)\right] \leq
  d \sqrt{L n_0}
\end{align*}
for any $n_0$, based on a similar analysis to ranked bandits. Let
\begin{align*}
  \Delta = \min_{i \in [K], t \in [n]} \left(\tilde{r}_t(i, J_\ast) - \max_{J:\, J \neq J_\ast} \tilde{r}_t(i, J)\right)
\end{align*}
be the minimum gap. Then, based on the above inequalities, the probability that the column learning algorithm chooses $J_\ast$ at any time $t \geq n_0$ is bounded from below by
\begin{align}
  1 - \frac{d \sqrt{L n_0}}{\Delta n_0} =
  \frac{\Delta \sqrt{n_0} - d \sqrt{L}}{\Delta \sqrt{n_0}}
  \label{eq:opt lower bound}
\end{align}
for any $\Delta \geq d \sqrt{L / n_0}$.

Let $p_t$ be the probability that the column learning algorithm chooses $J_\ast$ at time $t$ and let $\pi_{t, i}(J_\ast)$ be its permutation for user $i$ at time $t$, according to our row algorithm. Then we can bound the regret from time $n_0$ as
\begin{align*}
  R(n)
  & = \sum_{i = 1}^K \E\left[\sum_{t = n_0}^n 1\{i_t = i\}
  (r_t(i, \pi_{\ast, i_t}(J_\ast)) - r_t(i_t, \pi_{t, i_t}(J_t))\right] \\
  & = \sum_{i = 1}^K \E\left[\sum_{t = n_0}^n \frac{1}{p_t} 1\{i_t = i, J_t = J_\ast\}
  (r_t(i, \pi_{\ast, i_t}(J_\ast)) - r_t(i_t, \pi_{t, i_t}(J_\ast))\right] \\
  & \leq \left(1 + \frac{d \sqrt{L}}{\Delta \sqrt{n_0} - d \sqrt{L}}\right) \sum_{i = 1}^K R_i(n)\,,
\end{align*}
where $R_i(n)$ is the expected $n$-step regret of the row algorithm in row $i$, conditioned on the fact that the column learning algorithm chooses $J_\ast$. One suitable row algorithm would be the weighted majority algorithm, which learns the optimal permutation for each $J$. Then $R_i(n) = O(\log n + \log d!) \approx O(\log n + d \log d)$.

In the first $n_0$ steps, we bound the regret trivially by $n_0$. Then the expected $n$-step regret is bounded up to log factors as
\begin{align*}
  R(n) \leq
  n_0 + \left(1 + \frac{d \sqrt{L}}{\Delta \sqrt{n_0} - d \sqrt{L}}\right) K\,.
\end{align*}
The bound can be interpreted as follows. Choose some reasonable $n_0$ that makes the regret comparable to ranked bandits, such as $n_0 = c^2 d \sqrt{L n}$ for some $c > 0$. Then the multiplier at $K$ becomes smaller than $2$, for instance, when $\Delta c d^\frac{1}{2} L^\frac{1}{4} n^\frac{1}{4} \geq 2 d L^\frac{1}{2}$. Under the assumption that $n \geq L$, this happens when $\Delta \geq 2 \sqrt{d} / c$, which makes sense for $c > 2 \sqrt{d}$\,.
