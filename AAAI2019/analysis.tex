\begin{theorem}
\label{thm:LRB}
The worst case cumulative regret upper bound for latent ranker algorithm is,
\begin{align*}
R(n) \leq c^2d \sqrt{L n} + K \log n
\end{align*}

when $n > L$ and $c$ is a constant such that $c > 2\sqrt{d}$.
\end{theorem}

\begin{proof} \textbf{(Outline)}
The complete proof of Theorem \ref{thm:LRB} is given in Appendix \ref{sec:proof} and we sketch the main idea here.  We  express the total regret over all time steps as regret over time steps during which column algorithm suggests suboptimal arms and rest of the time steps. We can bound the contribution due to the former term as follows. The column learning algorithm has  low regret. This follows by an analysis similar to as in \cite{radlinski2008learning}. This implies that the column algorithm cannot suggest sub-optimal sets $J_t \neq J^*$ too often. 

The contribution to regret from the remaining time steps can be further decomposed as a sum over the users. We have a weighted majority algorithm for each user, and the regret bound follows by the standard guarantees as in \cite{WMA}.
\end{proof}

\begin{discussion}
\label{disc:proof1}
From the result in Theorem \ref{thm:LRB} we see that the regret consist of two parts of unequal order. The first part of order $O\left(d^2 \sqrt{L n} \right)$ for $c = 2\sqrt{d}$ is incurred for finding the $d$ best items (hott-topics) with high probability. The second part of order $O\left( K\log n\right)$ is incurred by WMA for finding the best permutation once the column MABs starts suggesting the $d$-best items. Note, that the result has the correct order as it \textit{does not} scale with $O\left(\sqrt{KLn}\right)$ like the independent user model algorithms.
\end{discussion}

\begin{discussion}
\label{disc:proof2}
Note, that for proving the regret bound we need an instance of LRA which uses EXP3 as column MAB. This is because the feedbacks are no longer independent of each other. The feedback $f_{k,t}$ to the $k$-th column MAB  is dependent on the feedback of the $k-1$-th column MAB. Hence, adversarial MABs which can work with any sequence of feedbacks are required for giving theoretical guarantees in this setting.
\end{discussion}
