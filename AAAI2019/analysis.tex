%!TEX root = paper.tex

\begin{theorem}
\label{thm:upper bound} Let $\colalg$ and $\rowalg$ in $\latentranker$ be $\expthree$ and the weighted majority algorithm, respectively. Then the expected $n$-step regret of $\latentranker$ is bounded as
\begin{align*}
  R(n) =
  O\left(\frac{d \sqrt{L n}}{\Delta} + K \log n + K d \log d\right)\,,
\end{align*}
where $\Delta = \min_{t \in [n]} \min_{J:\, J \neq J_\ast} \E\left[\tilde{r}_t(i, J_\ast)\right] - \E\left[\tilde{r}_t(i, J)\right]$ is an instance-specific lower bound on the gap in the unweighted expected rewards of the optimal and best suboptimal columns at any time $t \in [n]$, averaged over all users at that time.  

%\todob{We need to define the unweighted reward.}

\end{theorem}
\begin{proof}
The complete proof is in \cref{sec:proof}. We sketch it below. The main idea is to decompose the regret of $\latentranker$ into two parts, where $\colalg$ does not suggest $J_\ast$ and the rest.

The first part is analyzed as follows. $\colalg$ has a sublinear regret, based on a similar analysis to \citet{radlinski2008learning}. Therefore, our upper bound on the probability that $\colalg$ suggests suboptimal columns, which is $O(d \sqrt{L n} / \Delta)$, decreases with time horizon $n$.

The second part is analyzed as follows. Conditioned on $J_t = J_\ast$, the only remaining regret at time $t$ is due to the fact that columns $J_t$ are not ordered optimally for user $i_t$. Since this is a full-information problem where $\rowalg$ is the weighted majority algorithm \citep{littlestone1994weighted}, the regret due to learning to order $d$ columns for $K$ users is $K$ times that of the weighted majority algorithm with $d!$ arms.
\end{proof}

The regret in \cref{thm:upper bound} consists of two main parts. The first part, which is $O(d \sqrt{L n} / \Delta)$, is the regret due to learning the $d$ optimal hott-topics columns with a high probability. The second part, which is $O(K \log n + K d \log d)$, is due to learning the most rewarding permutations of the $d$ optimal columns for all users.

Our regret bound also improves upon a trivial approach where the optimal columns are learned separately for each user. If this problem was solved by RBA \citep{radlinski2008learning}, the regret would be $O(\sqrt{K L n})$.

Finally, we use non-stochastic algorithms for $\colalg$ and $\rowalg$ because our environment is non-stationary. In particular, we assume that user preferences $U_t$, and thus rewards, can change over time $t$. In addition, the rewards in $\colalg(k)$ are non-stationary due to chosen columns at higher positions $1, \dots, k - 1$.
