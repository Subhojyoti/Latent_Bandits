%!TEX root = paper.tex

\newcommand{\transpose}{^\mathsf{\scriptscriptstyle T}}

Let $[n] = \{1, \dots, n\}$ be the set of the first $n$ positive integers. For any two sets $A$ and $B$, we denote by $A^B$ the set of all vectors whose entries take values from $A$ and are indexed by $B$. Let $M$ be a $K \times L$ matrix. Then $M(I, :)$ is a $|I| \times L$ submatrix of $M$ whose rows are rows $I \subseteq [K]$ in $M$. Similarly, $M(:, J)$ is a $K \times |J|$ submatrix of $M$ whose columns are columns $J \subseteq [L]$ in $M$. Let $\Pi_d$ be the set of all $d$-permutations. For any $\pi \in \Pi_d$ and $d$-dimensional vector $v$, we denote by $v(\pi)$ the permutation of the entries of $v$ according to $\pi$. \todob{Lists should be treated as lists, not sets. We do not learn sets.}

We model user preferences by a family of low-rank matrices, which are known as hott topics. We define a \emph{hott-topics matrix} of rank $d$ as $M = U V\transpose$, where $U$ is a $K \times d$ non-negative matrix and $V$ is a $L \times d$ non-negative matrix that gives rise to the hott-topics structure. In particular, we assume that there exist $d$ rows $J_\ast$ in $V$ such that every row of $V$ can be written as a convex combination of rows $J_\ast$ and the zero vector. More precisely,
\begin{align}
  \forall j \in [L] \ \exists \alpha \in A: V(J_\ast, :) \alpha = V(j, :)\,,
  \label{eq:hott-topics}
\end{align}
where $A = \{a \in [0, 1]^{d \times 1}: \|a\|_1 \leq 1\}$.

In the context of user modeling, our factorization of $M$ can be interpreted as follows. The rank $d$ is the number of latent topics. The matrix $U$ are latent preferences of $K$ users over $d$ topics, where $U(i, :)$ are the preferences of user $i \in [K]$. Without loss of generality, we assume that $U \in [0, 1]^{L \times d}$. The matrix $V$ are latent coordinates of $L$ items in the space of $d$ topics, where $V(j, :)$ are the coordinates of item $j \in [L]$. Without loss of generality, we assume that the coordinates are points in a simplex, $\|V(j, :)\|_1 \leq 1$ for all $j \in [L]$.

We study the following online learning problem. At time $t$, the preferences of users are encoded in a $K \times L$ \emph{preference matrix} $r_t = U_t V\transpose$; where $U$ and $V$ are defined as above. We assume that user preferences $U_t$ can change arbitrarily in time. A random user $i_t \in [K]$ arrives to the recommender system at time $t$ and we recommend $d$ items $J_t$ to this user. The \emph{reward} for recommending these items is $r_t(i_t, J_t)$, where
\begin{align}
  r_t(i, J) =
  \max \, \{\mu(k) \, r_t(i, J(k)): k \in [d]\}
  \label{eq:reward}
\end{align}
is the reward for recommending items $J$ to user $i$ at time $t$, $J(k)$ is the $k$-th recommended item in $J$, and $\mu(k)$ is the preference for recommending at position $k \in [d]$. We assume that higher-ranked positions are more rewarding, $1 \geq \mu(1) \geq \dots \geq \mu(d) \geq 0$. The learning agent \emph{observes} the individual rewards of all recommended items, $r_t(i_t, J_t(k))$ for all $k \in [d]$. \todob{We need to motivate \eqref{eq:reward}. This should be the same motivation as in ranked bandits, except that $\mu$ enforces personalization, in the sense that the order matters.}

Since $U_t$ can change arbitrarily over time, the reward of items can change, and so does the most rewarding item for the user. Therefore, the problem of learning to recommend in this setting seems hard. A remarkable property of our user-item preference matrices is that for any $U_t \in [0, 1]^{L \times d}$ and any user $i \in [K]$,
\begin{align*}
  \argmax_{j \in [L]} r_t(i, j) \in J_\ast\,,
\end{align*}
where $J_\ast$ is defined in \eqref{eq:hott-topics}. Therefore, it is possible to learn the set of optimal items of all users statistically efficiently.

Now we are ready to define our notion of optimality and regret. Let $J_\ast$ be the hott-topics items in \eqref{eq:hott-topics} and $\pi_{\ast, i}$ be their permutation that maximizes the reward of user $i$ in hindsight,
\begin{align*}
  \pi_{\ast, i} =
  \argmax_{\pi \in \Pi_d} \sum_{t = 1}^n r_t(i, \pi(J_\ast))\,.
\end{align*}
Let $J_t$ be our recommended items at time $t$ and $\pi_{t, i}$ be their permutation for user $i$, which is computed by some later-defined algorithm (\cref{sec:algorithm}). Then our goal is to minimize the expected $n$-step regret,
\begin{align}
  R(n) =
  \sum_{t = 1}^n \E\left[r_t(i_t, \pi_{\ast, i_t}(J_\ast)) - r_t(i_t, \pi_{t, i_t}(J_t))\right]\,,
  \label{eq:regret}
\end{align}
where the expectation is with respect to both randomly arriving users and potential randomness in the learning algorithm.
