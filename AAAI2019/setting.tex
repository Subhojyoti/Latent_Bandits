%!TEX root = paper.tex

We study the following online learning problem, which we call a \emph{latent ranked bandit}. At time $t$, the preferences of users are encoded in a $K \times L$ \emph{preference matrix} $r_t = U_t V\transpose$; where $U$ and $V$ are defined as above. We assume that user preferences $U_t$ can change arbitrarily in time. A random user $i_t \in [K]$ arrives to the recommender system at time $t$ and we recommend $d$ items $J_t$ to this user. The \emph{reward} for recommending these items is $r_t(i_t, J_t)$, where
\begin{align}
  r_t(i, J) =
  \max \, \{\mu(k) \, r_t(i, J(k)): k \in [d]\}
  \label{eq:reward}
\end{align}
is the reward for recommending items $J$ to user $i$ at time $t$, $J(k)$ is the $k$-th recommended item in $J$, and $\mu(k)$ is the preference for recommending at position $k \in [d]$. We assume that higher-ranked positions are more rewarding, $1 \geq \mu(1) \geq \dots \geq \mu(d) \geq 0$. The learning agent \emph{observes} the individual rewards of all recommended items, $r_t(i_t, J_t(k))$ for all $k \in [d]$. \todob{We need to motivate \eqref{eq:reward}. This should be the same motivation as in ranked bandits, except that $\mu$ enforces personalization, in the sense that the order matters.}

Since $U_t$ can change arbitrarily over time, the reward of items can change, and so does the most rewarding item for the user. Therefore, the problem of learning to recommend in this setting seems hard. A remarkable property of our user-item preference matrices is that for any $U_t \in [0, 1]^{L \times d}$ and any user $i \in [K]$,
\begin{align*}
  \argmax_{j \in [L]} r_t(i, j) \in J_\ast\,,
\end{align*}
where $J_\ast$ is defined in \eqref{eq:hott-topics}. Therefore, it is possible to learn the set of optimal items of all users statistically efficiently.

Now we are ready to define our notion of optimality and regret. Let $J_\ast$ be the hott-topics items in \eqref{eq:hott-topics} and $\pi_{\ast, i}$ be their permutation that maximizes the reward of user $i$ in hindsight,
\begin{align*}
  \pi_{\ast, i} =
  \argmax_{\pi \in \Pi_d} \sum_{t = 1}^n r_t(i, \pi(J_\ast))\,.
\end{align*}
Let $J_t$ be our recommended items at time $t$ and $\pi_{t, i}$ be their permutation for user $i$, which is computed by some later-defined algorithm (\cref{sec:algorithm}). Then our goal is to minimize the expected $n$-step regret,
\begin{align}
  R(n) =
  \sum_{t = 1}^n \E\left[r_t(i_t, \pi_{\ast, i_t}(J_\ast)) - r_t(i_t, \pi_{t, i_t}(J_t))\right]\,,
  \label{eq:regret}
\end{align}
where the expectation is with respect to both randomly arriving users and potential randomness in the learning algorithm.
