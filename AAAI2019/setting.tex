%!TEX root = paper.tex

We study the following online learning problem, which we call a \emph{latent ranked bandit}. At time $t$, the preferences of users are encoded in a $K \times L$ \emph{preference matrix} $r_t = U_t V\transpose$; where $U$ and $V$ are defined as above. We assume that user preferences $U_t$ can change arbitrarily in time. A random user $i_t \in [K]$ arrives to the recommender system at time $t$ and we recommend $d$ items $J_t$ to this user. The \emph{reward} for recommending these items is $r_t(i_t, J_t)$, where
\begin{align}
  r_t(i, J) =
  \max \, \{\mu(k) \, r_t(i, J(k)): k \in [d]\}
  \label{eq:reward}
\end{align}
is the reward for recommending items $J$ to user $i$ at time $t$, $J(k)$ is the $k$-th recommended item in $J$, and $\mu(k)$ is the preference for recommending at position $k \in [d]$. \todoan{This doesn't result in decreasing order of preferences - if we get the max-weighted reward right, it doesn't matter how we order the remaining items. How about weighted sum of the rewards?}We assume that higher-ranked positions are more rewarding, $1 \geq \mu(1) \geq \dots \geq \mu(d) \geq 0$. The learning agent \emph{observes} the individual rewards of all recommended items, $r_t(i_t, J_t(k))$ for all $k \in [d]$. \todob{We need to motivate \eqref{eq:reward}. This should be the same motivation as in ranked bandits, except that $\mu$ enforces personalization, in the sense that the order matters.} \todoan{See the above comment. To motivate the fractional reward, how about saying that it can correspond to the length of the video the user watches. Like, we recommend a movie/video and if the user watches only half of the video, then the reward is 0.5.}

Since $U_t$ can change arbitrarily over time, the reward of items can change, and so does the most rewarding item for the user. Therefore, the problem of learning to recommend in this setting seems hard. A remarkable property of our user-item preference matrices is that for any $U_t \in [0, 1]^{L \times d}$ and any user $i \in [K]$,
\begin{align*}
  \argmax_{j \in [L]} r_t(i, j) \in J_\ast\,,
\end{align*}
where $J_\ast$ is defined in \eqref{eq:hott topics}. Therefore, it is possible to learn the set of optimal items of all users statistically efficiently.

Now we are ready to define our notion of optimality and regret. Let $J_\ast$ be the hott-topics items in \eqref{eq:hott topics} and $\pi_{\ast, i}$ be their permutation that maximizes the reward of user $i$ in hindsight,
\begin{align*}
  \pi_{\ast, i} =
  \argmax_{\pi \in \Pi_d} \sum_{t = 1}^n r_t(i, \pi(J_\ast))\,.
\end{align*}
Let $J_t$ be our recommended items at time $t$ and $\pi_{t, i}$ be their permutation for user $i$, which is computed by some later-defined algorithm (\cref{sec:algorithm}). Then our goal is to minimize the expected $n$-step regret,
\begin{align}
  R(n) =
  \sum_{t = 1}^n \E\left[r_t(i_t, \pi_{\ast, i_t}(J_\ast)) - r_t(i_t, \pi_{t, i_t}(J_t))\right]\,,
  \label{eq:regret}
\end{align}
where the expectation is with respect to both randomly arriving users and potential randomness in the learning algorithm.

\todob{We do not really need the greedy definition of $J_\ast$ until the proof.}
