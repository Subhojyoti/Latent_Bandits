%!TEX root = paper.tex

\newcommand{\transpose}{^\mathsf{\scriptscriptstyle T}}

Let $[n] = \{1, \dots, n\}$ be the set of the first $n$ positive integers. For any two sets $A$ and $B$, we denote by $A^B$ the set of all vectors whose entries take values from $A$ and are indexed by $B$. Let $M$ be a $K \times L$ matrix. Then $M(I, :)$ is a $|I| \times L$ submatrix of $M$ whose rows correspond to rows $I \subseteq [K]$ in $M$. Similarly, $M(:, J)$ is a $K \times |J|$ submatrix of $M$ whose columns correspond to columns $J \subseteq [L]$ in $M$.

We model user preferences by a family of low-rank matrices, which are known as hott-topics \cite{}. We define a \emph{hott-topics matrix} of rank $d$ as $M = U D V\transpose$, where $U$ is a $K \times d$ non-negative matrix, $D$ is a non-negative $d \times d$ diagonal matrix, and $V$ is a $L \times d$ non-negative matrix that gives rise to the hott-topics structure. In particular, we assume that there exist $d$ rows $J_\ast$ of $V$ such that each row of $V$ can be written as a convex combination of rows $J_\ast$ and the zero vector. More precisely,
\begin{align}
  \forall j \in [L] \ \exists \alpha \in A: V(J_\ast, :) \alpha = V(j, :)\,,
  \label{eq:hott topics}
\end{align}
where $A = \{a \in [0, 1]^{d \times 1}: \|a\| \leq 1\}$.

In the context of user modeling, our decomposition of $M$ can be viewed as follows. The rank $d$ is the number of latent topics. The matrix $U$ represents preferences of $L$ users over $d$ topics, such as expected ratings. Without loss of generality, we assume that $U \in [0, 1]^{L \times d}$. The matrix $V$ represents the coordinates of $L$ items in the space of $d$ topics. Without loss of generality, we assume that the coordinates are points in a simplex, $\|V(j, :)\| \leq 1$ for all $j \in [L]$. Finally, we use $D$ to model changing topic preferences over time, such as that some topic becomes suddenly more preferred than others. Without loss of generality, we assume that $D \in [0, 1]^{d \times d}$.

We study the following online learning problem. At time $t$, the preferences of users for items are encoded in a $K \times L$ \emph{reward matrix} $r_t = U D_t V\transpose$; where $U$, $D$, and $V$ are defined as above. We assume that matrix $D_t$ can change arbitrarily in time. A random user $i_t \in [K]$ arrives to the recommender system at time $t$ and we recommend $d$ items $J_t$ to this user. The \emph{reward} for these items is $r_t(i_t, J_t)$, where
\begin{align*}
  r_t(i, J) =
  \max \, \{\mu(k) \, r_t(i, J(k)): k \in [d]\}
\end{align*}
is the reward for recommending items $J$ to user $i$ at time $t$, $J(k)$ is the $k$-th recommended item in $J$, and $\mu(k)$ is the preference for recommending an item at position $k \in [d]$. We assume that higher-ranked positions are more rewarding, $1 \geq \mu(1) \geq \dots \geq \mu(d) \geq 0$. The learning agent \emph{observes} the individual rewards of all recommended items, $r_t(i_t, J_t(k))$ for all $k \in [d]$.

Since $D_t$ can change arbitrarily over time, the reward of any item can vary over time; and the most rewarding item for each user can also change. A remarkable property of our matrices is that for any $D_t$ and any user $i$,
\begin{align*}
  \argmax_{j \in [L]} r_t(i, j) \in J_\ast\,,
\end{align*}
where $J_\ast$ is defined in \eqref{eq:hott topics}. Therefore, we can learn statistically efficiently the set of potentially optimal items on average over all users.

Now we are ready to define our notion of optimality and regret. Let $J_\ast$ be the hott items in \eqref{eq:hott topics} and $\pi_{\ast, i}$ be their permutation that maximizes the reward of user $i$ in hindsight. Let $J_t$ be our recommended items at time $t$ and $\pi_{t, i}$ be their permutation for user $i$, which is computed by some later-defined algorithm (\cref{sec:algorithm}). Then our goal is to minimize the expected $n$-step regret,
\begin{align*}
  R(n) =
  \sum_{t = 1}^n \E\left[r_t(i_t, \pi_{\ast, i_t}(J_\ast)) - r_t(i_t, \pi_{t, i_t}(J_t))\right]\,,
\end{align*}
where the expectation is with respect to randomly arriving users and potential randomness in the learning algorithm.
