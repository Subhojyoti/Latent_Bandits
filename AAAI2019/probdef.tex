Let $[n] := \lbrace 1,2,\ldots, n\rbrace$. For any two sets $A$ and $B$, $A^B$ denotes the set of all vectors indexed by $B$ and whose coordinates are in $A$. Let $M\in [0,1]^{K\times L}$ be a matrix and $I \subset [K] $, then $M(I,:)$ denotes the $|I| \times L$ dimensional submatrix of $M$ corresponding to the rows whose indices are given by $I$. Similarly, we use $M(:,J)$ to denote the submatrix of $M$ whose columns are given by $J$.
	
	Let $M \in  [0,1]^{K \times L}$  reward matrix.. Also, let us assume that the rank of  $M$ is $d \ll \min\lbrace L,K\rbrace$ \todoan{Always use $\ll$ instead of $<<$}.  \todo{Changed} Let $ U \in [ \mathbb{R}^+ ]^{K\times d} \textbf{, } V \in  [0,1]^{L\times d}$ be matrices representing the user and item latent factors, 
\begin{align*}
	M = UV^{\intercal} .
\end{align*}	  
	
	Furthermore, we put a constraint on $V$ such that, $\forall j\in [L]$, $ \norm{V(j,:)}_1 \leq 1$. \todoan{Do we need this assumption??}
	
	%\textbf{(Hott-Topics)}
	
\begin{assumption}\textbf{(Hott-Topics)}
\label{assm:hott-topics}
We assume that there exists $d$ rows $J^*$,  such that every row of $V$ can be written as a convex combination of the rows in $V(J^*,:)$ and the zero vector. 
\end{assumption}
We denote the column \todoan{should be row} factors by $V^* = V(J^*,:)$. Therefore, for every $i\in [L]$, there exists a column vector $a \in [0,1]^{d}$ and $ \norm{a}_1 \leq 1$ such that 
\begin{align*}
V(i,:) = V(J^*,:) a.
\end{align*}


%In this paper, in addition to the noisy setting explained in section \ref{intro} we first analyze the proposed algorithm in the easier noise free setting. In the noise free setting, the nature reveals the row $i_t$, and when the learner selects the column $j_t$, it observes the mean of the distribution $\bar{R}(i_t,j_t)$.

%\begin{assumption}
%\label{assm:round-robin}
%We assume that nature is revealing the user $i$ in $\bar{R}(i,:), \forall i\in [K]$  in a Round-Robin fashion such that at timestep $t$, nature reveals $i_t = (t \mod K) + 1$.
%\end{assumption}

\begin{assumption}\textbf{(Click Model)}
\label{assm:click-model}

\todoan{Don't have this as an assumption. After the notations, explain precisely what is the observation model and noise model in 1-2 paragraphs. This will be part of that.}

For each user $i_t$ revealed by the nature at round $t$, the learner is allowed to suggest atmost $d$-items, where $d$ is the rank of the matrix $M$. The user can click one, or all, or none of the recommendations and the learner observes all the $d$ clicks. We assume a Document Click Model (DCM) such that every user-item pair has a single parameter called the user-item attraction factor which determines the click probability of the user when the item is shown.  

\todosb{Moved as observation model}
\end{assumption}

\begin{discussion}
The above Assumption \ref{assm:click-model} is an instance of the \textit{Document Click Model (DCM)} first studied in \citet{craswell2008experimental}. Since, this click model depends on learning only one attraction factor for each user-item pair, it often leads to overfitting of model parameters. DCM is independent on the position of the item (PBM) and does not model the decreasing interest of the user (CBM) while surveying an ordered set of  items. 
\todoan{You can comment this out from here.}
\todosb{Will remove this.}
\end{discussion}

%In DCM every user-item pair has a single parameter called the user-item attraction factor which determines the click probability of the user when the item is shown
%Assumption \ref{assm:click-model} can be conceptualized in the real-world scenario where the learner has  to suggest movies to users and each movie belongs to a different genre (say thriller, romance, comedy, etc). So, the learner can suggest $d$ movies belonging to different genres to each user in a webpage, and the user can click one, or all, or none of the recommended movies (query abandonement).

\textbf{Observation Model:} For each user $i_t$ revealed by the nature at round $t$, the learner is allowed to suggest atmost $d$-items, where $d$ is the rank of the matrix $M$. The user can click one, or all, or none of the recommendations and the learner observes all the $d$ clicks. We assume a Document Click Model (DCM) \citep{craswell2008experimental} such that every user-item pair has a single parameter called the user-item attraction factor which determines the click probability of the user when the item is shown.  

\textbf{Noise Model:}  At every timestep $t$, we generate a noisy matrix $\tilde{M}_t = UD_t V^{\intercal}$, where $D_t$ is a diagonal matrix such that $D_t(i,i)\in[0,1]$. Thus, for every such realization of $\tilde{M}_t$ \todoan{remove $\forall t in [n]$, you are saying for every $t$ at the start of the paragraph} the hott-topics structure of $M$ is preserved. \todosb{removed $\forall t$}

\begin{discussion}
Our noise model is quite different from the existing stochastic noise model assumptions of various click models. The usual i.i.d Bernoulli reward assumption on the entries of the user-item preference matrix $M$ is not feasible because the hott-topics assumption is required for every realization of the matrix $M$. 
\end{discussion}

\todoan{Describe our noise model first. Any discussions and comments justifying it should come after precisely explaining the observation and noise model}.

\todosb{added as discussion}


The main goal of the learning agent is to minimize the cumulative regret $\mathcal{R}_n$  over $n$ time steps

\begin{align*}
\mathcal{R}_n = \sum_{t=1}^{n}\bigg\lbrace \sum_{z=1}^{d} \bigg( r_{t}\left(i_{t}, j^* \right) - r_{t}\left( i_{t}, j_{t,z}\right)\bigg)\bigg\rbrace.
\end{align*}
Here, $j^* = \argmax_{j\in [L]}\lbrace M(i_t,j)\rbrace$ \todoan{Why is there $i_t$ here? $j^*$ is independent of $t$} and $j_{t,z}$ be the suggestion of the learner for the $i_t$ -th user for  $z=1,2,\ldots, d$ \todoan{This gives the same reward for all permutations}. Note that $r_{t}\left(i_t, j^* \right)\sim \tilde{M}_t\left(i_t, j^*\right)$ and $r_{t}\left(i_t, j_{t,z} \right)\sim \tilde{M}_t\left(i_t, j_{t,z} \right)$. Taking expectation over both sides, we can show that,

\begin{align*}
\E[\mathcal{R}_n] & = \E\left [ \sum_{t=1}^{n}\bigg\lbrace\sum_{z=1}^{d} \bigg( r_{t}\left(i_t, j^* \right) - r_{t}\left( i_t, j_{t,z}\right)\bigg)\bigg\rbrace\right] \\
%%%%%%%%%%%%%%%%%%%%
&= \E\left [ \sum_{t=1}^{n} \sum_{z=1}^{d} \bigg( N_{i_t,j_{z,t}}(t)\bigg) \right ]\Delta_{i_t,j_{t,z}}
\end{align*}

where, $\Delta_{i_t,j_{z,t}} = M(i_t,j^*) - M(i_t,j_{z,t})$ and $N_{i_t,j_{t,z}}(t)$ is the number of times the learner has observed the $j_{t,z}$-th item for the $i_t$-th user. Let, $\Delta = \min_{i\in[K],j\in[L]}\lbrace \Delta_{i,j}\rbrace$ be the minimum gap over all the user, item pair in $M$.
