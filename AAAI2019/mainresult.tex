\begin{customlemma}{2}
\label{psbandit:Lemma:1}
Let, $\mu_{i,g}$ be the expected mean of an arm $i$ for the piece $\rho_{g}$, $N_{i,t_s:t}$ be the number of times an arm $i$ is pulled from $t_s$ till the $t$-th timestep such that $t>t_{g}$, then at the $t$-th timestep for all $\delta\in (0,1]$  it holds that,
\begin{align*}
\Pb\bigg\lbrace\forall & t'\in [t_s , t]: \big(\hat{\mu}_{i,t_s:t'} - S_{i,t_s:t'} > \hat{\mu}_{i,t'+1:t} + S_{i,t'+1:t}\big) \bigcup\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 &\big(\hat{\mu}_{i,t_s:t'} +  S_{i,t_s:t'} < \hat{\mu}_{i,t'+1:t} - S_{i,t'+1:t} \big)\bigg\rbrace \leq \delta
\end{align*}

where $S_{i,t_s:t'} = \sqrt{\dfrac{\log(\frac{4t^2}{\delta})}{2N_{i,t_s:t'}}}$.

\end{customlemma}

\begin{customproof}{2} \textbf{Outline} We first define the bad event $\xi^{chg}_{i,t}$ (see equation \ref{event:out:1}) which determines whether the changepoint is detected or not. Next, we use Chernoff-Hoeffding inequality along with union bound to bound the probability of this bad event $\xi^{chg}_{i,t}$.
\begin{eqnarray}
&\xi^{chg}_{i,t} = \bigg\lbrace\forall t'\in [t_s , t]: \big(\hat{\mu}_{i,t_s:t'} - S_{i,t_s:t'}  \nonumber\\ 
%%%%%%%%%%%%%%%%%
&\leq \hat{\mu}_{i,t'+1:t} + S_{i,t'+1:t}\big) \bigcup \big(\hat{\mu}_{i,t_s:t'} +  S_{i,t_s:t'} \geq  \nonumber\\
%%%%%%%%%%%%%%%%
 & \hat{\mu}_{i,t'+1:t} - S_{i,t'+1:t}\big)\bigg\rbrace. \label{event:out:1}
\end{eqnarray}

The proof of Lemma \ref{psbandit:Lemma:1} is given in Appendix \ref{sec:proof:Lemma:1}.
\end{customproof}

\begin{remark}
\label{Rem:1}
Choosing $\delta=\dfrac{1}{t}$, the above result of Lemma \ref{psbandit:Lemma:1} can be reduced to $\Pb\lbrace \xi^{chg}_{i,t}\rbrace \leq \dfrac{4}{t}$, where the event $\xi^{chg}_{i,t}$ is identical to Eq (\ref{event:out:1}). Note, that $\delta$ does not depend on the knowledge of horizon $T$.
\end{remark}


\begin{customtheorem}{1}
\label{psbandit:Theorem:1}
The expected cumulative regret of UCB-CPD (Algorithm \ref{alg:UCBCPD}) using the CPD (Algorithm \ref{alg:CPD}) is given by,

\begin{align*}
\E[R_t]  &\leq \sum_{i=1}^{K}\sum_{g=1}^{G} \bigg\lbrace 3 + \dfrac{8\log(t)}{\Delta^{opt}_{i,g}} + \dfrac{\pi^2}{3} + \dfrac{6\Delta^{opt}_{i,g}\log(t)}{(\Delta^{chg}_{i,g})^2} \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%
& + \dfrac{6\Delta^{opt}_{\max,g+1}\log(t)}{(\Delta^{chg}_{\epsilon_0,g})^{2}} + \dfrac{12\Delta^{opt}_{i,g+1}\log(t)}{(\Delta^{chg}_{i,g})^{2}}\bigg\rbrace.
\end{align*}

\end{customtheorem}

\begin{customproof}{3} \textbf{(Outline)}
We follow the standard approach of proving the regret bound for UCB algorithms, whereby we first bound the probability of the bad event of pulling the sub-optimal arms between two changepoints and the number of pulls required for each sub-optimal arm after which they can be discarded. Similarly, we bound the probability of the bad event of not detecting a changepoint and the minimum number of pulls required to detect a  changepoint of sufficient gap (see Lemma \ref{psbandit:Lemma:1}). In all the cases above, we use Chernoff-Hoeffding inequality to bound the probability of the bad events. The proof of Theorem \ref{psbandit:Theorem:1} is given in Appendix \ref{sec:proof:Theorem:1}.
\end{customproof}


\begin{customtheorem}{2}
\label{psbandit:Theorem:2}
The expected cumulative regret of ImpCPD (Algorithm \ref{alg:ImpCPD}) using CPDI (Algorithm \ref{alg:CPDI}) is upper bounded by,
\begin{align*}
& \E[R_T] \leq \sum_{i\in\A'}\sum_{j=1}^{G}\bigg[ 1 
%%%%%%%%%%
+  \dfrac{8C\left(\gamma, \alpha \right)\Delta^{opt}_{i,g}\log(\psi T )}{(\psi T^{-\frac{3}{2\alpha}})^{\alpha}}  + \Delta^{opt}_{i,g}\\
%%%%%%%%%%
&  + \dfrac{8\log(\psi T(\Delta^{opt}_{i,g})^4)}{(\Delta^{opt}_{i,g})}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
 +   \dfrac{8C\left(\gamma, \alpha \right)\Delta^{opt}_{i,g}\log(\psi T )}{(\psi T^{-\frac{3}{2\alpha}})^{\alpha}}  \bigg]\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%
& + \sum_{i\in\A}\sum_{j=1}^{G}\bigg[\Delta^{opt}_{\max,g+1} + \dfrac{8\Delta^{opt}_{\max,g+1}\log(\psi T(\Delta^{chg}_{i,g})^4)}{(\Delta^{chg}_{\epsilon_0,g})^2}\bigg]\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%
& + \sum_{i\in\A'}\sum_{j=1}^{G}\bigg[\Delta^{opt}_{i,g+1} + \dfrac{8\Delta^{opt}_{i,g+1}\log(\psi T(\Delta^{chg}_{i,g})^4)}{(\Delta^{chg}_{i,g})^2}
\bigg]
\end{align*}

where $\alpha,\psi,\gamma$ are exploration parameters, $C\left( \gamma,\alpha\right)=\left( \frac{1+\gamma}{\gamma}\right)^{2\alpha + 1}$, $\A'=\big\lbrace i\in\A: \Delta^{opt}_{i,g}\geq \sqrt{\frac{e}{T}}, \Delta^{chg}_{i,g}\geq \sqrt{\frac{e}{T}},\forall g\in\G \big\rbrace$ and $\Delta^{opt}_{i,{G+1}} = 0,\forall i\in\A$.
\end{customtheorem}

\begin{customproof}{4} \textbf{(Outline)}
We divide the proof into two larger modules, where in the first module, we bound the optimality regret of not pulling the optimal arm between two changepoints $g-1$ to $g$ using steps $3,4$ and $5$. In the second module we bound the changepoint regret incurred for not detecting the $g$-th changepoint using steps $2,6$ and $7$. We use Chernoff-Hoeffding inequality to bound the probability of the bad events, and we control the number of pulls of each sub-optimal arm using the definition of $\ell_{m_i}$ and exploration parameters $\alpha,\psi,\gamma$. The proof of Theorem \ref{psbandit:Theorem:2} is given in Appendix \ref{sec:proof:Theorem:2}.
\end{customproof}

\begin{customcorollary}{1}
\label{psbandit:Corollary:1}
In the worst case scenario, when all the gaps are same, that is $\Delta^{opt}_{i,g}=\Delta^{chg}_{i,g}=\Delta^{chg}_{\epsilon_0,g}=\sqrt{\frac{K\log K}{T}}, \forall i\in\A,\forall g\in\G$ and $\delta=\dfrac{1}{T}$ then the worst case gap-independent regret bound of UCB-CPD is given by,

\begin{align*}
\E[R_{T}] \leq 3KG + \dfrac{KG \pi^2}{3} + \dfrac{32G\sqrt{KT}\log T}{\sqrt{{\log K}}}.
\end{align*}


\end{customcorollary}

\begin{customproof}{5}
The proof of Corollary \ref{psbandit:Corollary:1} is given in Appendix \ref{sec:proof:Corollary:1}.
\end{customproof}

\begin{discussion}
\label{dis:Corollary:1}
From the result in Corollary \ref{psbandit:Corollary:1}, we see that the largest contributing factor to the expected regret of  UCB-CPD is of the order $O\left( G\sqrt{T}\log T\right)$.
\end{discussion}



\begin{customcorollary}{2}
\label{psbandit:Corollary:2}
In the worst case scenario, when all the gaps are same, that is $\Delta^{opt}_{i,g}=\Delta^{chg}_{i,g}=\Delta^{chg}_{\epsilon_0,g}=\sqrt{\frac{K\log K}{T}}, \forall i\in\A,\forall g\in\G$ and setting $\alpha=1.5$, $\psi = \frac{T}{K^2\log K}$ and $\gamma = 0.05$ then the worst case gap-independent regret bound of ImpCPD is given by,

\begin{align*}
\E[R_T]&\leq 3G\sqrt{\dfrac{K^3\log K}{T}} + C_1 GK^{4.5}(\log K)^{2} + 48G\sqrt{KT},
\end{align*}

where $C_1$ is an integer constant.

\end{customcorollary}

\begin{customproof}{6}
The proof of Corollary \ref{psbandit:Corollary:2} is given in Appendix \ref{sec:proof:Corollary:2}.
\end{customproof}

\begin{discussion}
\label{dis:Corollary:2}
From the result in Corollary \ref{psbandit:Corollary:2}, we see that the largest contributing factor to the expected regret of ImpCPD is of the order $O\left( G\sqrt{T}\right)$. This is lower than the regret upper bound of DUCB, SWUCB, Exp3.R and  CUSUM-UCB (see table \ref{tab:comp-bds}).
\end{discussion}

\begin{discussion}
\label{dis:Corollary:21}
Also from the result in Corollary \ref{psbandit:Corollary:2}, we see that smaller the value of $\gamma$ the larger is the constant $C1$ associated with the factor $GK^{4.5}(\log K)^{2}$. Note, that $\gamma$ determines how frequently the CPDI is called by ImpCPD and by modifying the confidence interval we have been able to increase the probability of detecting the changepoint at the cost of additional regret that only scales with $K$ and not with $T$.
\end{discussion}


%\begin{remark}
%\label{Rem:2}
%Comparing the result of Corollary \ref{psbandit:corollary:1} and Theorem \ref{Theorem:5} we see that Algorithm \ref{alg:SECPD2} incurs more regret of $\left( \dfrac{K(1+\eta+B)\Delta_{i,g}'\log(t\sqrt{\log(t)})}{\Delta_{i,}^2} \right)$ against Algorithm \ref{alg:SECPD1}. But this comes against a decrease in computation in Algorithm \ref{alg:SECPD2} against Algorithm \ref{alg:SECPD1}. Note that for each $t\in[t_0,t]$ where $t_0,t\in[1,T]$, for detecting changepoint, Algorithm \ref{alg:SECPD1} has to do a computation of the order of $O\left( K(t - t_0)^2\right)$ whereas for Algorithm \ref{alg:SECPD2} it is $O\left( K(t - t_0)\dfrac{\log(t - t_0)}{\log B}\right)$ where $B>1$.
%\end{remark}