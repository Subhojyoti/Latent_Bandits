%!TEX root = bandit_paper.tex

Now, we propose the general \emph{Low Rank Bandit ($\latentranker$)} algorithm for solving the family of non-stochastic, non-negative and low-rank matrices of rank $d$. The pseudocode of rank-$d$ $\latentranker$ is in \cref{alg:LRB1} and the details are as follows. $\latentranker$ has two main components, column learning and row learning algorithm. 
%\todob{There is no single matrix.}

The row learning algorithm is $d$ instances of multi-armed bandit algorithms, which we denote by $\rowalg(k)$ algorithm for $k \in [d]$. At every round $t$, the row learning algorithm recommends a list of $d$ rows. Similarly, there are $d$ instances of $\colalg(k)$ algorithm for $k \in [d]$ and it recommends a list of $d$ columns. If there is a conflict of suggestion between $\rowalg$s, then another unique row is suggested and added to the clashed row set $S_r$. Similarly, if there is conflict of suggestion between $\colalg$s, then another unique column is suggested and added to the clashed column set  $S_c$. Finally, $\latentranker$ observes the individual rewards of $M_t(i,j)$ for all $(i,j)\in (I_t, J_t)$. Then we update both column and row learning algorithms. For $k_1 \in [d]$, the reward of the arm $i_{k_1}$ in $\rowalg(k_1)$, which selects the $i_{k_1}$-th row in $I_t$, is updated as follows. If the $i_{k_1}$-th arm was not one of the previously suggested rows its reward is updated $d$ times such that, $\max_{k \leq k_1} M_t(i_k, j_{k_2}) - \max_{k < k_1} M_t(i_k, j_{k_2})$ for all $k_1,k_2 \in [d]$. By the choice of our design and previous argument, for $k_2 \in [d]$ a similar update is performed on the $j_{k_2}$-th column by $\colalg(k_2)$ such that its reward is also updated $d$ times with rewards, $\max_{k \leq k_2} M_t(i_{k_1}, j_k)  - \max_{k < k_2} M_t(i_{k_1}, j_k)$ for all $k_1,k_2 \in [d]$. Otherwise, if any of the row or column has been previously selected by the corresponding $\rowalg$ or $\colalg$ algorithm (present in $S_r$ or $S_c$) then we update it with reward $0$.

%by exploiting the same structure in our rewards
%But  we exploit an additional structure (eq \eqref{eq:rewardProp}) in our problem to learn the optimal rows $I^\ast$.
The design of our algorithm is motivated from the assumptions in  \eqref{eq:rankd} and \eqref{eq:rewardProp} that the maximum entry of $M_{t}$ will always lie in the $(I^\ast, J^\ast)$ for all round $t\in [n]$. So, the goal is to identify the maximum entry of the matrix by quickly identifying the $d$-best rows or columns. But an issue with learning in this setting is that we have to maximize over a set function which cannot be solved computationally efficiently as we have to learn over all combinations of $d\times d$ rows and columns. To resolve this, we use a chain of $\rowalg(k)$ and $\colalg(k)$ for $k=1,2,\ldots,d$ which learns the $d$-best members of this set function. So, the first $\rowalg(1)$ learns the best row based on the highest increment in reward, then $\rowalg(2)$ learns the second best row based on the second highest increment and so on. A similar learning happens for the $\colalg(k)$, $k\in [d]$ because the hott-topics assumption is both on $U$ and $V$ (eq \eqref{eq:hott topics1}). When we run both $\rowalg$ and $\colalg$ in parallel, and the distributions in the other dimensions do not change too fast (this is true by our assumption in eq \eqref{eq:rankd}, \eqref{eq:rewardProp}), then $\rowalg(k)$ and $\colalg(k)$, $k\in [d]$ would slowly converge to the $d$ hott-topic rows and columns.  If there is conflict of suggestion for either rows or columns, only one $\rowalg$ or $\colalg$ is updated so that each can focus on a distinct row or column respectively. A similar type of update is also done in a much simpler setting of Ranked Bandit Algorithm (RBA) in \citet{radlinski2008learning}.


%Over all $t\in [n]$, the $\rowalg(1)$ learns the most rewarding row on average, $\rowalg(2)$ learns the second most rewarding row on average conditioned on the first learned row, and so on. 
%\todoan{These are hand-wavy statements at best, and doesn't make much sense mathematically. Don't say RowAlg(2) learns something without knowing that it does. Same applies below.}
%and is the same as the Ranked Bandit Algorithm (RBA) in \citet{radlinski2008learning}. \todob{Leave talking about related work for ``Related Work''. It is distracting if you do it here.}
%show \todoan{We don't 'show' anything for rank-$d$ case.} that we
% Again the goal of the column learning algorithm $\colalg$ is to learn the optimal set of columns $J^\ast$. Hence, over all $t\in [n]$, $\colalg(1), \dots, \colalg(d)$ learns the most rewarding columns in $J^*$ on average. Note that this sequence of learning the rows or columns first does not matter because the \emph{hott-topics} structure is defined on both $U$ and $V$ matrix (eq \eqref{eq:hott topics1}) generating $M_t = U_t V_t^\intercal$, and so we will be learning the $d$-best rows or columns in average. When we run both $\rowalg$ and $\colalg$ in parallel, and the distributions in the other dimensions do not change too fast (this is true by our assumption in eq \eqref{eq:rankd}, \eqref{eq:rewardProp}), then $i_1, \dots, i_d$ and $j_1, \dots, j_d$ would slowly converge to the $d$ hott-topic rows and columns.
%This type of update makes sure that the $\rowalg(k)$ or $\colalg(k)$ learns the $k$-th best row or column conditioned on the selection of previous $1,\ldots, k-1$ $\rowalg$ or $\colalg$ respectively.

%Another way of looking at this is to first realize that if we fix the column selection strategy, which is simply some distribution over $d$-tuples of chosen columns then for any such distribution, the $d$ hott-topic rows are the optimal solution to the row selection problem. By symmetry, the same is true for the column selection problem. If we run both in parallel, and the distributions in the other dimensions do not change too fast (this is true by our design in eq \eqref{eq:rewardProp}), then $i_1, \dots, i_d$ and $j_1, \dots, j_d$ would slowly converge to the $d$ hott-topic rows and columns. \todoan{I find these statements to be ambiguously between rigorous mathematical statements and informal intuition. You have to make it clear that it is the latter and choose words appropriately.}

%\todob{This needs to be stated formally. What does it mean mathematically?}
%\todob{This is not the notation for the best columns.}
%Here is an informal argument for the correctness of the algorithm. Fix the column selection strategy, which is simply some distribution over d-tuples of chosen columns. For any such distribution, the d hott topic rows are the optimal solution to the row selection problem. By symmetry, the same is true for the column selection problem. If we run both in parallel, and the distributions in the other dimensions do not change too fast (this is true by our design), I would think that i_1, \dots, i_d and j_1, \dots, j_d would slowly converge to the d hott topic rows and columns.
% \todoan{The above has lot of mistakes. I think you are mixing up k, $k_1$ and $k_2$. Please rewrite the above 3 sentences. Fix the issues pointed out in the algorithm first.}
%\todob{Explain why this is needed.}
%The reward of the arm in $\colalg(k)$, which selects the $k$-th column in $J_t$, is updated as follows. If the arm was not one of the previously suggested columns, its reward is $\max \, \{M_t(i_t, J_t(a)): a \in [k]\} - \max \, \{M_t(i_t, J_t(a)): a \in [k - 1]\}$. Otherwise, we update the initially suggested arm with reward $0$. Since $\latentranker$ observes the individual rewards of all recommended items, we can compute the reward of any permutation of $J_t$ in row $i_t$. These rewards are then used to update $\rowalg(i_t, J_t)$. 
%It then observes the reward $u_t(i_t)v_t(j_t)$. The $\colalg$ learns the most rewarding column on average, while $\rowalg$ learns the most rewarding row on average. Once it observes the reward it updates the statistics of each algorithm with the reward $u_t(i_t)v_t(j_t)$.

%The column learning algorithm recommends a list of $d$ columns and is the same as in \citet{radlinski2008learning}. But we exploit an additional structure in our problem to show that we learn the optimal columns $J_\ast$. The column learning algorithm are $d$ instances of multi-armed bandit algorithms, which we denote by $\colalg(k)$ for algorithm $k \in [d]$. $\colalg(1)$ learns the most rewarding column on average, $\colalg(2)$ learns the second most rewarding column on average conditioned on the first learned column, and so on.
%
%The row ranking algorithm permutes columns suggested by the column learning algorithm. It consists of multiple instances of full-information algorithms. More precisely, for each user $i \in [K]$ and set of $d$ columns $J$, we have algorithm $\rowalg(i, J)$ with $d!$ arms, which correspond to all possible permutations of $J$. The objective of $\rowalg(i, J)$ is to learn a permutation of $J$ with the highest reward, as measured by \eqref{eq:reward}.
%
%$\latentranker$ interacts with the environment as follows. At time $t$, a random user $i_t$ is revealed to $\latentranker$. Then, in the ascending order of $k \in [d]$, $\colalg(k)$ suggests column $\ell_k$. If $\colalg(k)$ suggests one of the previously suggested columns $\ell_1, \dots, \ell_{k - 1}$, then $\ell_k$ is chosen uniformly at random from the remaining columns. We denote the vector of $d$ suggested columns  by $J_t$. Then $\rowalg(i_t, J_t)$, the row learning algorithm for user $i_t$ and columns $J_t$, selects permutation $\pi_{t, i_t}$ of $J_t$.
%
%The user is recommended a permuted list $\pi_{t, i_t}(J_t)$ and $\latentranker$ observes the individual rewards of all recommended items. Then we update both column and row learning algorithms. The reward of the arm in $\colalg(k)$, which selects the $k$-th column in $J_t$, is updated as follows. If the arm was not one of the previously suggested columns, its reward is $\max \, \{M_t(i_t, J_t(a)): a \in [k]\} - \max \, \{M_t(i_t, J_t(a)): a \in [k - 1]\}$. Otherwise, we update the initially suggested arm with reward $0$. Since $\latentranker$ observes the individual rewards of all recommended items, we can compute the reward of any permutation of $J_t$ in row $i_t$. These rewards are then used to update $\rowalg(i_t, J_t)$. 
\begin{algorithm}[!ht]
  \caption{Low Rank Bandit ($\latentranker$) (Rank-$d$)}
  \label{alg:LRB1}
  \begin{algorithmic}[1]
    \State \textbf{Input:} Time horizon $n$, Rank $d$
    \For{$k = 1, \dots, d$}
    \Comment{Initialization}
      \State Initialize $\rowalg (k)$
      \State Initialize $\colalg (k)$
    \EndFor
    \For{$t = 1, \dots, n$}
    \State Initialize Clashed Row Set $S_r$ = $\{\emptyset\}$
    \State Initialize Clashed Column Set $S_c$ = $\{\emptyset\}$
    \For{$k = 1, \dots, d$}
      \Comment{Generate Row response}
        \State $\hat{i}_k \gets$ Suggested row $i_t$ by $\rowalg(k)$
        %\todob{$\hat{b}$ for rows is very confusing. What about $\hat{i}$? The same goes for $b$.}
        \If{$\hat{i}_k \in \{i_1, \dots, i_{k - 1}\}$}
          \State $i_k \gets$ Random row not in $\{i_1, \dots, i_{k - 1}\}$
          \State Add $i_k$ to $S_r$
        \Else
          \State $i_k \gets \hat{i}_k$
        \EndIf
        \EndFor
        \For{$k = 1, \dots, d$}
      \Comment{Generate Column response}
        \State $\hat{j}_k \gets$ Suggested column $j_t$ by $\colalg(k)$
        %\todob{$\hat{\ell}$ for columns is very confusing. What about $\hat{j}$? The same goes for $\ell$.}
        \If{$\hat{j}_k \in \{j_1, \dots, j_{k - 1}\}$}
          \State $j_k \gets$ Random column not in $\{j_1, \dots, j_{k - 1}\}$
          \State Add $j_k$ to $S_c$
        \Else
          \State $j_k \gets \hat{j}_k$
        \EndIf
      \EndFor
      \State $I_t \gets (i_1, \dots, i_d)$
      \State $J_t \gets (j_1, \dots, j_d)$
      \State Observe $M_t(I_t(k), J_t(k))$ for all $k \in [d]$
      \For{$k_1 = 1, \dots, d$}
      \Comment{Update statistics}
      \For{$k_2 = 1, \dots, d$}
        \If{$i_{k_1} \notin S_r$} 
        \State Update arm $\! i_{k_1}\!$ of $\!\rowalg(k_1)\!$ with reward
       \begin{align*}
    &\qquad \qquad \max_{k \leq k_1} M_t(i_k, j_{k_2}) - \max_{k < k_1} M_t(i_k, j_{k_2})
        \end{align*}
        \Else
          \State Update $\hat{i}_{k_1}$ of $\rowalg(k_1)$ with reward $0$
        \EndIf
        \If{$j_{k_2} \notin S_c$}
    		\State Update arm $\!j_{k_2}\!$ of $\!\colalg(k_2)\!$ with reward 
         \begin{align*}
         &\qquad \qquad \max_{k \leq k_2} M_t(i_{k_1}, j_k)  - \max_{k < k_2} M_t(i_{k_1}, j_k)
        \end{align*}
        \Else
          \State Update $\hat{j}_{k_2}$ of $\colalg(k_2)$ with reward $0$
        \EndIf
      \EndFor
      \EndFor
     \EndFor
  \end{algorithmic}
\end{algorithm}
%\todoan{What is $k$? You are looping over $k_1$ and $k_2$.}
% \todoan{This is all messed up. You are using $k$ as an argument in $\max$ and also in the sentence above.}
\subsection{Practical Considerations}
\label{sec:practical considerations}
%The proposed $\latentranker$ algorithm only has to update/look through $(Kd + d)$ items for each of the $d$ $\colalg$ and the i-th $\rowalg$ at every timestep $t$. This is in stark contrast to some of the existing matrix completion algorithms which has to reconstruct a $K\times L$ matrix \citep{sen2016contextual} or calculate second or third order tensors \citep{gopalan2016low}. 
%\todob{Discuss time and space complexities of $\latentranker$.}
%\todob{Say how we hack $\rowalg$. 
%We do not have one row Exp3 for each user and any combination of columns, right? %This is how LRA is proposed. This needs to be said.}
%$\latentranker$-rank$2$
%We leave the implementation of the $\colalg$ and $\rowalg$ to the users.  
Motivated by rank-$1$, we use a non-stochastic algorithm $\expthree$ as $\colalg$ and $\rowalg$, respectively. For experimental purposes, stochastic algorithms like $\ucb$ or Thompson Sampling can also be used to improve the performance of $\latentranker$. This has also been explored in \citet{radlinski2008learning} where $\RBA$ uses $\ucb$ for ranking items. The proposed $\latentranker$ algorithm only updates $(K + L)d$ entries for $d$ instances of $\colalg$ and $\rowalg$, respectively, at every round $t$. This is in stark contrast to some of the existing matrix completion algorithms, which reconstruct a $K\times L$ matrix \citep{sen2016contextual} or decompose tensors \citep{gopalan2016low}. 
%and showed how the regret scales for the rank-$1$ setting
%For theoretical guarantees \todob{What guarantees? We have none. Say that this is motivated by rank $1$.}
%\todob{I am pretty sure that you can use just one of these words. Why would you say update/look?}
%This is ok. I will write about col Exp3 and row WMA and why they are needed. Great. Also mention that previous works considered UCB1 and we do that as well.

%\todob{Discuss suitable choices for $\colalg$ and $\rowalg$.}
