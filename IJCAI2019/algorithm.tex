%!TEX root = bandit_paper.tex

Now, we propose the general \emph{Low Rank Bandit ($\latentranker$)} algorithm for solving the family of non-stochastic, non-negative and low-rank matrices of rank $d$. Again, the goal is to identify the maximum entry of the matrix by quickly identifying the $d$-best rows or columns. \todob{There is no single matrix.} The pseudocode of $\latentranker$ is in \cref{alg:LRB1}. $\latentranker$ has two main components, column learning and row learning algorithm. 

At every timestep \todob{round} $t$, the row learning algorithm recommends a list of $d$ rows and is the same as the Ranked Bandit Algorithm (RBA) in \citet{radlinski2008learning}. \todob{Leave talking about related work for ``Related Work''. It is distracting if you do it here.} But we exploit an additional structure in our problem to show that we learn the optimal rows $I^\ast$. The row learning algorithm are $d$ instances of multi-armed bandit algorithms, which we denote by $\rowalg(k)$ for algorithm $k \in [d]$. $\rowalg(1)$ learns the most rewarding row on average, $\rowalg(2)$ learns the second most rewarding row on average conditioned on the first learned column, and so on.

Similarly, column learning algorithm recommends a list of $d$ columns by exploiting the same structure in our rewards. Again the goal of the column learning algorithm $\colalg$ is to learn the optimal set of columns $J^\ast$. Hence, $\colalg(1), \dots, \colalg(d)$ learns the most rewarding columns $j_1, \dots, j_d$ \todob{This is not the notation for the best columns.} on average. Note, that this sequence of learning the rows or columns first does not matter because the \emph{hott-topics} structure is defined on both $U$ and $V$ matrix generating $M_t = U_t V_t^\intercal$, and so we will be learning the $d$-best rows or columns in average. \todob{This needs to be stated formally. What does it mean mathematically?} Another way of looking at this is to first realize that if we fix the column selection strategy, which is simply some distribution over $d$-tuples of chosen columns then for any such distribution, the $d$ hott-topic rows are the optimal solution to the row selection problem. By symmetry, the same is true for the column selection problem. If we run both in parallel, and the distributions in the other dimensions do not change too fast (this is true by our design), then $i_1, \dots, i_d$ and $j_1, \dots, j_d$ would slowly converge to the $d$ hott-topic rows and columns.


%Here is an informal argument for the correctness of the algorithm. Fix the column selection strategy, which is simply some distribution over d-tuples of chosen columns. For any such distribution, the d hott topic rows are the optimal solution to the row selection problem. By symmetry, the same is true for the column selection problem. If we run both in parallel, and the distributions in the other dimensions do not change too fast (this is true by our design), I would think that i_1, \dots, i_d and j_1, \dots, j_d would slowly converge to the d hott topic rows and columns.


Finally, $\latentranker$ observes the individual rewards of $M(i,j)$ for all $(i,j)\in (I_t, J_t)$. Then we update both column and row learning algorithms. The reward of the arm in $\rowalg(k)$, which selects the $k$-th row in $I_t$, is updated as follows. If the $k$-th arm was not previously suggested row its reward is updated $d$ times such that, $\max_{k \leq k_1} M_t(i_k, j_{k_2}) - \max_{k < k_1} M_t(i_k, j_{k_2})$ for all $k_1,k_2 \in [d]$. By the choice of our design and previous argument a similar update is performed on the $k$-th column learning algorithm $\colalg$ such that its reward is also updated $d$ times such that, $\max_{k \leq k_2} M_t(i_{k_1}, j_k)  - \max_{k < k_2} M_t(i_{k_1}, j_k)$ for all $k_1,k_2 \in [d]$. Otherwise, if any of the row or column has been previously selected by the corresponding $\rowalg$ or $\colalg$ algorithm then we update it with reward $0$. \todob{Explain why this is needed.}


%The reward of the arm in $\colalg(k)$, which selects the $k$-th column in $J_t$, is updated as follows. If the arm was not one of the previously suggested columns, its reward is $\max \, \{M_t(i_t, J_t(a)): a \in [k]\} - \max \, \{M_t(i_t, J_t(a)): a \in [k - 1]\}$. Otherwise, we update the initially suggested arm with reward $0$. Since $\latentranker$ observes the individual rewards of all recommended items, we can compute the reward of any permutation of $J_t$ in row $i_t$. These rewards are then used to update $\rowalg(i_t, J_t)$. 


%It then observes the reward $u_t(i_t)v_t(j_t)$. The $\colalg$ learns the most rewarding column on average, while $\rowalg$ learns the most rewarding row on average. Once it observes the reward it updates the statistics of each algorithm with the reward $u_t(i_t)v_t(j_t)$.

%The column learning algorithm recommends a list of $d$ columns and is the same as in \citet{radlinski2008learning}. But we exploit an additional structure in our problem to show that we learn the optimal columns $J_\ast$. The column learning algorithm are $d$ instances of multi-armed bandit algorithms, which we denote by $\colalg(k)$ for algorithm $k \in [d]$. $\colalg(1)$ learns the most rewarding column on average, $\colalg(2)$ learns the second most rewarding column on average conditioned on the first learned column, and so on.
%
%The row ranking algorithm permutes columns suggested by the column learning algorithm. It consists of multiple instances of full-information algorithms. More precisely, for each user $i \in [K]$ and set of $d$ columns $J$, we have algorithm $\rowalg(i, J)$ with $d!$ arms, which correspond to all possible permutations of $J$. The objective of $\rowalg(i, J)$ is to learn a permutation of $J$ with the highest reward, as measured by \eqref{eq:reward}.
%
%$\latentranker$ interacts with the environment as follows. At time $t$, a random user $i_t$ is revealed to $\latentranker$. Then, in the ascending order of $k \in [d]$, $\colalg(k)$ suggests column $\ell_k$. If $\colalg(k)$ suggests one of the previously suggested columns $\ell_1, \dots, \ell_{k - 1}$, then $\ell_k$ is chosen uniformly at random from the remaining columns. We denote the vector of $d$ suggested columns  by $J_t$. Then $\rowalg(i_t, J_t)$, the row learning algorithm for user $i_t$ and columns $J_t$, selects permutation $\pi_{t, i_t}$ of $J_t$.
%
%The user is recommended a permuted list $\pi_{t, i_t}(J_t)$ and $\latentranker$ observes the individual rewards of all recommended items. Then we update both column and row learning algorithms. The reward of the arm in $\colalg(k)$, which selects the $k$-th column in $J_t$, is updated as follows. If the arm was not one of the previously suggested columns, its reward is $\max \, \{M_t(i_t, J_t(a)): a \in [k]\} - \max \, \{M_t(i_t, J_t(a)): a \in [k - 1]\}$. Otherwise, we update the initially suggested arm with reward $0$. Since $\latentranker$ observes the individual rewards of all recommended items, we can compute the reward of any permutation of $J_t$ in row $i_t$. These rewards are then used to update $\rowalg(i_t, J_t)$. 


\begin{algorithm}[t]
  \caption{Low Rank Bandit ($\latentranker$) (Rank-$d$)}
  \label{alg:LRB1}
  \begin{algorithmic}[1]
    \State \textbf{Input:} Time horizon $n$, Rank $d$
    \For{$k = 1, \dots, d$}
    \Comment{Initialization}
      \State Initialize $\rowalg (k)$
      \State Initialize $\colalg (k)$
    \EndFor
    \For{$t = 1, \dots, n$}
    \For{$k = 1, \dots, d$}
      \Comment{Generate response}
        \State $\hat{i}_k \gets$ Suggested row $i_t$ by $\rowalg(k)$
        %\todob{$\hat{b}$ for rows is very confusing. What about $\hat{i}$? The same goes for $b$.}
        \If{$\hat{i}_k \in \{i_1, \dots, i_{k - 1}\}$}
          \State $i_k \gets$ Random row not in $\{i_1, \dots, i_{k - 1}\}$
        \Else
          \State $i_k \gets \hat{i}_k$
        \EndIf
        \State $\hat{j}_k \gets$ Suggested column $j_t$ by $\colalg(k)$
        %\todob{$\hat{\ell}$ for columns is very confusing. What about $\hat{j}$? The same goes for $\ell$.}
        \If{$\hat{j}_k \in \{j_1, \dots, j_{k - 1}\}$}
          \State $j_k \gets$ Random column not in $\{j_1, \dots, j_{k - 1}\}$
        \Else
          \State $j_k \gets \hat{j}_k$
        \EndIf
      \EndFor
      \State $I_t \gets (i_1, \dots, i_d)$
      \State $J_t \gets (j_1, \dots, j_d)$
      \State Observe $M_t(I_t(k), J_t(k))$ for all $k \in [d]$
      \For{$k_1 = 1, \dots, d$}
      \For{$k_2 = 1, \dots, d$}
      \Comment{Update statistics}
        \If{$i_{k} = \hat{i}_{k}$}
        \State Update arm $i_{k}$ of $\rowalg(k)$ with reward
       \begin{align*}
    &\qquad \qquad \max_{k \leq k_1} M_t(i_k, j_{k_2}) - \max_{k < k_1} M_t(i_k, j_{k_2})
        \end{align*}
        \Else
          \State Update $\hat{i}_k$ of $\rowalg(k)$ with reward $0$ 
        \EndIf
        \If{$j_{k} = \hat{j}_{k}$}
    		\State Update arm $j_{k}$ of $\colalg(k)$ with reward
         \begin{align*}
         &\qquad \qquad \max_{k \leq k_2} M_t(i_{k_1}, j_k)  - \max_{k < k_2} M_t(i_{k_1}, j_k)
        \end{align*}
        \Else
          \State Update $\hat{j}_k$ of $\colalg(k)$ with reward $0$
        \EndIf
      \EndFor
      \EndFor
     \EndFor
  \end{algorithmic}
\end{algorithm}

%\todob{Simplify the presentation of the update below. To start with, break it into row and column components.}

\subsection{Practical Considerations}
\label{sec:practical considerations}

%The proposed $\latentranker$ algorithm only has to update/look through $(Kd + d)$ items for each of the $d$ $\colalg$ and the i-th $\rowalg$ at every timestep $t$. This is in stark contrast to some of the existing matrix completion algorithms which has to reconstruct a $K\times L$ matrix \citep{sen2016contextual} or calculate second or third order tensors \citep{gopalan2016low}. 
%\todob{Discuss time and space complexities of $\latentranker$.}


%\todob{Say how we hack $\rowalg$. 
%We do not have one row Exp3 for each user and any combination of columns, right? %This is how LRA is proposed. This needs to be said.}
%$\latentranker$-rank$2$

We leave the implementation of the $\colalg$ and $\rowalg$ to the users. For theoretical guarantees \todob{What guarantees? We have none. Say that this is motivated by rank $1$.} we use non-stochastic algorithm $\expthree$ as $\colalg$ and $\rowalg$ and showed how the regret scales for the rank-$1$ setting. For experimental purposes, stochastic algorithms like $\ucb$ or Thompson Sampling can also be used to improve the performance of $\latentranker$. This has also been explored in \citet{radlinski2008learning} where $\RBA$ uses $\ucb$ for ranking items. The proposed $\latentranker$ algorithm only has to update/look \todob{I am pretty sure that you can use just one of these words. Why would you say update/look?} through $(K + L)d$ entries for the $d$ $\colalg$ and the $\rowalg$ respectively at every round $t$. This is in stark contrast to some of the existing matrix completion algorithms which has to reconstruct a $K\times L$ matrix \citep{sen2016contextual} or calculate second or third order tensors \citep{gopalan2016low}. 

%This is ok. I will write about col Exp3 and row WMA and why they are needed. Great. Also mention that previous works considered UCB1 and we do that as well.

%\todob{Discuss suitable choices for $\colalg$ and $\rowalg$.}
