%!TEX root = bandit_paper.tex

In this section, we study the online learning problem of finding the maximum entry of a non-stochastic, low-rank and non-negative matrix for the general rank-$d$ setting. 

\textbf{Hott-topics Assumption:} We focus on a family of low-rank matrices, which are known as hott topics. We define a \emph{hott-topics matrix} of rank $d$ as $M = U V\transpose$, where $U$ is a $K \times d$ non-negative matrix and $V$ is a $L \times d$ non-negative matrix that gives rise to the hott-topics structure. In particular, we assume that there exists $d$ rows $I^\ast$ in $U$ such that each row in $U$ can be represented as a convex combination of rows of $I^\ast$ and the zero vector. Hence, for an $A = \{a \in [0, 1]^{d \times 1}: \|a\|_1 \leq 1\}$ each row of $U$ can be expressed as,
\begin{align}
  \forall i \in [K] \ \exists \alpha \in A: U(I^\ast, :) \alpha = U(i, :)\,,
  \label{eq:hott topics1}
\end{align}
Similarly, we assume that there exist $d$ rows $J^\ast$ in $V$ such that each row of $V$ can be expressed as a convex combination of rows $J^\ast$ and the zero vector,
\begin{align}
  \forall j \in [L] \ \exists \alpha \in A: V(J^\ast, :) \alpha = V(j, :)\,,
  \label{eq:hott topics}
\end{align}
where $A = \{a \in [0, 1]^{d \times 1}: \|a\|_1 \leq 1\}$. Hence, the matrix $M$ represents preferences of users for items, $M(i, j)$ is the preference of user $i$ for item $j$. The rank $d$ of $M$ is the number of latent topics. The matrix $U$ are latent preferences of $K$ users over $d$ topics, where $U(i, :)$ are the preferences of user $i \in [K]$. The matrix $V$ are latent preferences of $L$ items in the space of $d$ topics, where $V(j, :)$ are the coordinates of item $j \in [L]$. Without loss of generality, we assume that $U \in [0, 1]^{K \times d}$ and $V \in [0, 1]^{L \times d}$. We assume that the coordinates are points in a simplex, that is $\|U(i, :)\|_1 \leq 1$ for all $i \in [K]$ and $\|V(j, :)\|_1 \leq 1$ for all $j \in [L]$. Note that our assumptions imply that $M(i, j) \geq 0$ for any $i \in [K]$ and $j \in [L]$.


\textbf{Rank-$d$ Setting:} Again, note that at time $t$, the preferences of users over items are encoded in a $K \times L$ \emph{preference matrix} $M_t = U_t V_t\transpose$, where $U_t$, and $V_t$ are defined as in \eqref{eq:hott topics1} and \eqref{eq:hott topics}. We assume that user and item preferences ($U_t$ and $V_t$ respectively) can change with time $t$. 
At every round $t$ the learner chooses $d$-pairs of rows and columns from $M_t$ denoted by $(I_t,J_t)\in \Pi_d([K])\times \Pi_d([L])$. It then observes all the values from the matrix $M_{t}(I_t,J_t)$ for all $i_t\in I_t$ and $j_t \in J_t$. The \emph{reward} for the agent for choosing arms $(I_t,J_t)$ at time $t$ is denoted by $r_t(i^\ast(I_t,J_t),j^\ast(I_t,J_t))$ such that,
\begin{align}
  (i^\ast(I,J),j^\ast(I,J)) = \argmax_{(i,j) \in (I\times J)} M_{t}(i,j)
  \label{eq:reward}
\end{align}

%a noisy \todob{We do not have any noise, right?} realization of

\todob{Say how the hott topics assumption simplifies the problem of finding the maximum entry of a matrix. Write it formally.}

%We study an online learning to rank problem, which we call a \emph{latent ranked bandit}. At time $t$, the preferences of users are encoded in a $K \times L$ \emph{preference matrix} $M_t = U_t V\transpose$, where $M$, $U_t$, and $V$ are defined as in \cref{sec:background}. We assume that user preferences $U_t$ can change with time $t$. A random user $i_t \in [K]$ arrives to the recommender system at time $t$ and we recommend $d$ items $J_t$ to this user. The \emph{reward} for recommending these items is $r_t(i_t, J_t)$, where
%\begin{align}
%  r_t(i, J) =
%  \max \, \{\mu(k) \, M_t(i, J(k)): k \in [d]\}
%  \label{eq:reward}
%\end{align}
%is the reward for recommending items $J$ to user $i$ at time $t$, $J(k)$ is the $k$-th item in $J$, and $\mu(k)$ is the weight of position $k \in [d]$. We assume that higher-ranked positions are more rewarding, $1 \geq \mu(1) \geq \dots \geq \mu(d) \geq 0$. The learning agent \emph{observes} the individual rewards of all recommended items, $M_t(i_t, J_t(k))$ for all $k \in [d]$.

%\todob{We need to motivate \eqref{eq:reward} from the user-modeling point of view. This should be the same motivation as in ranked bandits, except that $\mu$ enforces personalization, in the sense that the order matters.} \todoan{See the above comment. To motivate the fractional reward, how about saying that it can correspond to the length of the video the user watches. Like, we recommend a movie/video and if the user watches only half of the video, then the reward is 0.5.}

Since $U_t$ can change arbitrarily over time \todob{This obviously cannot be arbitrary. The assumption is that all $M_t$ have the same hott topics rows and columns. Write it formally.}, the reward in \eqref{eq:reward} is maximized by lists $J$ with highly rewarding items that are diverse, in the sense that they attain high rewards at different times $t \in [n]$. A remarkable property of our user-item preference matrices $M_t$ is that for any user $i \in [K]$ and any item $j \in [L]$ at any time $t$,
\begin{align*}
  \argmax_{(i, j) \in ([K] \times [L])} M_t(i, j) \in (I^\ast, J^\ast),
\end{align*}
where $I^\ast$ and $J^\ast$ is defined in \eqref{eq:hott topics1} and \eqref{eq:hott topics}. Therefore, it is possible to learn all potentially most rewarding pairs of rows and columns statistically efficiently.

%\todob{The definition of the regret below is incorrect for $d > 1$. We need to think about this. Actually, do we need the definition of the regret for $d > 1$ if we never bound it? We should use determinants and then intuitively explain what they mean. The rank $1$ case is intuitive and easy to explain.}

\textbf{Regret Definition (Rank-$d$):} Now we are ready to define our notion of optimality and regret for the general rank-$d$ scenario. Our goal is to minimize the expected $n$-step regret,
\begin{align}
  R(n) =
  \sum_{t = 1}^n \E\left[r_t(i^\ast_t, j^\ast_t) - r_t(i^\ast(I,J),j^\ast(I,J))\right]\,,
  \label{eq:regret1}
\end{align}
where the expectation is with respect to both randomly choosing rows $(I_t)$ and columns $(J_t)$ by the learning algorithm and potential randomness in the environment.

%Now we are ready to define our notion of optimality and regret. Let $J_\ast$ be the hott-topics items in \eqref{eq:hott topics} and $\pi_{\ast, i}$ be their permutation that maximizes the reward of user $i$ in hindsight,
%\begin{align*}
%  \pi_{\ast, i} =
%  \argmax_{\pi \in \Pi_d} \sum_{t = 1}^n r_t(i, \pi(J_\ast))\,.
%\end{align*}
%Let $J_t$ be our recommended items at time $t$ and $\pi_{t, i}$ be their permutation for user $i$, both of which are learned. Then our goal is to minimize the expected $n$-step regret,
%\begin{align}
%  R(n) =
%  \sum_{t = 1}^n \E\left[r_t(i_t, \pi_{\ast, i_t}(J_\ast)) - r_t(i_t, \pi_{t, i_t}(J_t))\right]\,,
%  \label{eq:regret}
%\end{align}
%where the expectation is with respect to both randomly arriving users and potential randomness in the learning algorithm.

%\todob{We do not really need the greedy definition of $J_\ast$ until the proof.}
