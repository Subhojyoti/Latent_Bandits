%!TEX root = bandit_paper.tex

In this section, we study the online learning problem of finding the maximum entry of a family of non-stochastic, low-rank and non-negative matrices for the general rank-$d$ setting.  
%\todob{You make it sound like there is only one matrix. This is false.}

\textbf{Hott-topics Assumption:} We focus on a family of low-rank matrices, which are known as hott topics \citep{recht2012factoring}. We define a \emph{hott-topics matrix} of rank $d$ as $M = U V\transpose$, where $U$ is a $K \times d$ non-negative matrix and $V$ is a $L \times d$ non-negative matrix that both have the hott-topics structure. We say a matrix  $U$  has hott-topics structure if  there exists $d$ rows  $U$ whose indices are $I^\ast$ such that each row in $U$ can be represented as a convex combination of these rows  and the zero vector. Hence, for $A = \{a \in [0, 1]^{d \times 1}: \|a\|_1 \leq 1\}$, each row of $U$ can be expressed as
\begin{align}
  \forall i \in [K] \ \exists \alpha \in A: U(I^\ast, :) \alpha = U(i, :)\,,
  \label{eq:hott topics1}
\end{align}
We also assume that there exist $d$ rows in $V$ whose indices are $J^\ast$ such that each row of $V$ can be expressed as a convex combination of these rows  and the zero vector.
%\begin{align}
  %\forall j \in [L] \ \exists \alpha \in A: V(J^\ast, :) \alpha = V(j, :)\,,
  %\label{eq:hott topics}
%\end{align}
%where $A = \{a \in [0, 1]^{d \times 1}: \|a\|_1 \leq 1\}$.
As an illustrative example, we can consider that the matrix $M = UV^\intercal$ as a low-rank matrix of rank $d$. The rank $d$ of $M$ is the number of latent topics. $M(i, j)$ is the reward of row $i$ and column $j$. $U$ are latent vectors of $K$ rows over $d$ topics, where $U(i, :)$ are the coordinates of row $i \in [K]$. The matrix $V$ are latent vectors of $L$ columns in the space of $d$ topics, where $V(j, :)$ are the coordinates of column $j \in [L]$. 
%\todob{The text below should be an illustrative example. You write this as if hott topics were synonymous to latent topics. They are not. This is just to show the reviewer that what we have can be used to model something.} $M(i, j)$ is the reward of row $i$ and column $j$. The rank $d$ of $M$ is the number of latent topics.  \todob{End of the illustrative example.}
Without loss of generality, we assume that $U \in [0, 1]^{K \times d}$ and $V \in [0, 1]^{L \times d}$.
%\todob{We do not need the next assumptions. This was for determinants in the stochastic setting.} We assume that the coordinates are points in a simplex, that is $\|U(i, :)\|_1 \leq \!1$ for all $i \in [K]$ and $\|V(j, :)\|_1 \!\leq\! 1$ for all $j \in [L]$. So, our assumptions imply that $\!M(i, j) \!\geq\! 0$ for any $i \!\in \![K]$ and $j \!\in\! [L]$.
%\todob{This is a strange transition. You start talking about rows, columns, and topics like this is synonymous to hott topics. This is false. This is just an illustration of what hott topics can mean in practice, right?}

\textbf{Rank-$d$ Setting:} Let for all rounds $t\in[n]$, $U_t \in [0,1]^{K\times d}$, $V_t \in [0,1]^{L\times d}$ and $M_t = U_t V_t^{\intercal}$. We also assume that for all rounds $t\in [n]$, the rows of $U_t$ can be written as a convex combination of the rows of $U_t(I^\ast, :)$ and the rows of $V_t$ can be written as a convex combination of the rows of $V_t(J^\ast, :)$. This implies that for all round $t\in [n]$,
%Similarly to \eqref{eq:rank1ij} we assume that for all round $t\in [n]$,
\begin{align}
& \argmax_{i\in[K]}M_{t}(i,j) \in I^\ast, \text{ for all $j \in [L]$} , \nonumber\\
& \argmax_{j\in[L]}M_{t}(i,j)\in J^\ast , \text{ for all $i \in [K]$}\label{eq:rankd}
\end{align}
%\todob{This argument should be flipped. We assume that all $M_t$ are hott topics, with the same $I^\ast$ and $J^\ast$. This implies the above.}
Hence, assumption \eqref{eq:rankd} on reward matrices $M_t$ ensures that for all round $t \in [n]$,
\begin{align}
  \argmax_{(i, j) \in [K] \times [L]} M_t(i, j) \in (I^\ast, J^\ast) \label{eq:rewardProp},
\end{align}
%for any row $i \in [K]$, any column $j \in [L]$ \todob{$i$ and $j$ are in the max. Why do you say that this holds for any $i$ and $j$? Only $t$.} and
%\todoan{The above makes no sense. $U_{t}(I,:)$ is a matrix and not scalar. What do you mean by arg-max??}
%as defined in \eqref{eq:hott topics1} and \eqref{eq:hott topics}. \todoan{You have already defined what hott-topics is. Why are you repeating it here?}
%and \eqref{eq:hott topics}
where $I^\ast$ and $J^\ast$ is defined in \eqref{eq:hott topics1}. The hott-topics assumption makes it possible to learn the maximum entry of $M_t$ statistically efficiently at any round $t\in[n]$ because the maximum entry $M_t(i^*,j^*)$ will be in $M_t(I^*,J^*)$. Note that even though different entries of $U_t$ and $V_t$ can attain high rewards at different times, $I^*$ and $J^*$ remain fixed for all round $t\in[n]$ (eq \ref{eq:rankd}). 
%, for some fixed $I^\ast \in [K]^d$ and $J^\ast \in [L]^d$
%. where $U$ and $V$ are defined as in \eqref{eq:hott topics1} and \eqref{eq:hott topics}. 
%Again, note that at time $t$, the preferences of rows over columns are encoded in a $K \times L$ \emph{preference matrix} $M_t = U_t V_t\transpose$. We further assume (similar to \eqref{eq:rank1ij}) that for all rounds $t\in [n]$, the rows of $U_t$ can be written as a convex combination of the rows of $U_t(I^\ast, :)$ and the rows of $V_t$ can be written as a convex combination of the rows of $V_t(J^\ast, :)$, for some fixed $I^\ast \in [K]^d$ and $J^\ast \in [L]^d$. where $U$ and $V$ are defined as in \eqref{eq:hott topics1} and \eqref{eq:hott topics}. 
At every round $t$, the learner chooses $d$-tuples of rows and columns from $M_t$ denoted by $(I_t,J_t)\in \Pi_d([K])\times \Pi_d([L])$, where $\Pi_{d}([K])$ and $\Pi_{d}([L])$ as defined in  \cref{sec:background}.  %\todob{Have we defined $\Pi_d([K])$ before?} 
It then observes all the values from the matrix $M_{t}(I_t,J_t)$ for all $i_t\in I_t$ and $j_t \in J_t$. The \emph{reward} for the agent for choosing arms $(I_t,J_t)$ at round $t$ is denoted by $M_t(i^\ast(I_t,J_t),j^\ast(I_t,J_t))$ such that,
\begin{align}
  (i^\ast(I,J),j^\ast(I,J)) = \argmax_{(i,j) \in (I\times J)} M_{t}(i,j)
  \label{eq:reward}
\end{align}
%We also need to clearly say how this guarantees that \arg\max_i M_t(i, j) \in I^\ast for any j and t. This is a formal way of saying that the best row is in I^\ast for any column in any round. We also have that \arg\max_j M_t(i, j) \in J^\ast for any i and t.


%hott-topics assumption makes sure that
%\todoan{It is not hott topics structure that ensures this. This is an extra assumption we make.}

%\todob{You need to watch what you write. $U$ and $V$ are defined in \eqref{eq:hott topics1} and \eqref{eq:hott topics}. Not $U_t$ and $V_t$. Now you need to say what the properties of $U_t$ and $V_t$ are. You need to state that the best rows and columns in $M_t$ remain the same.}
%the reward in \eqref{eq:reward} is maximized by lists $J$ with highly rewarding columns that are diverse, in the sense that they attain high rewards at different times $t \in [n]$. 

%todob{This obviously cannot be arbitrary. The assumption is that all $M_t$ have the same hott topics rows and columns. Write it formally.},
%\todob{Say how the hott topics assumption simplifies the problem of finding the maximum entry of a matrix. Write it formally.}

%We study an online learning to rank problem, which we call a \emph{latent ranked bandit}. At time $t$, the preferences of rows are encoded in a $K \times L$ \emph{preference matrix} $M_t = U_t V\transpose$, where $M$, $U_t$, and $V$ are defined as in \cref{sec:background}. We assume that row preferences $U_t$ can change with time $t$. A random row $i_t \in [K]$ arrives to the recommender system at time $t$ and we recommend $d$ columns $J_t$ to this row. The \emph{reward} for recommending these columns is $r_t(i_t, J_t)$, where
%\begin{align}
%  r_t(i, J) =
%  \max \, \{\mu(k) \, M_t(i, J(k)): k \in [d]\}
%  \label{eq:reward}
%\end{align}
%is the reward for recommending columns $J$ to row $i$ at time $t$, $J(k)$ is the $k$-th column in $J$, and $\mu(k)$ is the weight of position $k \in [d]$. We assume that higher-ranked positions are more rewarding, $1 \geq \mu(1) \geq \dots \geq \mu(d) \geq 0$. The learning agent \emph{observes} the individual rewards of all recommended columns, $M_t(i_t, J_t(k))$ for all $k \in [d]$.

%\todob{We need to motivate \eqref{eq:reward} from the row-modeling point of view. This should be the same motivation as in ranked bandits, except that $\mu$ enforces personalization, in the sense that the order matters.} \todoan{See the above comment. To motivate the fractional reward, how about saying that it can correspond to the length of the video the row watches. Like, we recommend a movie/video and if the row watches only half of the video, then the reward is 0.5.}

%Since $U_t$ can change arbitrarily over time \todob{This obviously cannot be arbitrary. The assumption is that all $M_t$ have the same hott topics rows and columns. Write it formally.}, the reward in \eqref{eq:reward} is maximized by lists $J$ with highly rewarding columns that are diverse, in the sense that they attain high rewards at different times $t \in [n]$. 

%\todob{The definition of the regret below is incorrect for $d > 1$. We need to think about this. Actually, do we need the definition of the regret for $d > 1$ if we never bound it? We should use determinants and then intuitively explain what they mean. The rank $1$ case is intuitive and easy to explain.}

\textbf{Regret Definition (Rank-$d$):} Now we are ready to define our notion of optimality and regret for the general rank-$d$ scenario. Our goal is to minimize the expected $n$-step regret,
\begin{align}
  R(n) =
  \sum_{t = 1}^n \E\left[M_t(i^\ast_t, j^\ast_t) - M_t(i^\ast(I,J),j^\ast(I,J))\right]\,,
  \label{eq:regret1}
\end{align}
where the expectation is with respect to both randomly choosing rows $(I_t)$ and columns $(J_t)$ by the learning algorithm and potential randomness in the environment.

%Now we are ready to define our notion of optimality and regret. Let $J_\ast$ be the hott-topics columns in \eqref{eq:hott topics} and $\pi_{\ast, i}$ be their permutation that maximizes the reward of row $i$ in hindsight,
%\begin{align*}
%  \pi_{\ast, i} =
%  \argmax_{\pi \in \Pi_d} \sum_{t = 1}^n r_t(i, \pi(J_\ast))\,.
%\end{align*}
%Let $J_t$ be our recommended columns at time $t$ and $\pi_{t, i}$ be their permutation for row $i$, both of which are learned. Then our goal is to minimize the expected $n$-step regret,
%\begin{align}
%  R(n) =
%  \sum_{t = 1}^n \E\left[r_t(i_t, \pi_{\ast, i_t}(J_\ast)) - r_t(i_t, \pi_{t, i_t}(J_t))\right]\,,
%  \label{eq:regret}
%\end{align}
%where the expectation is with respect to both randomly arriving rows and potential randomness in the learning algorithm.

%\todob{We do not really need the greedy definition of $J_\ast$ until the proof.}
