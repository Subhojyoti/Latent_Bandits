In this section, we compare $\latentranker$ to several bandit algorithms in three experiments. The first two experiments are on synthetic dataset where all modeling assumptions hold. The third experiment is on a real-life dataset where we evaluate $\latentranker$ when our modeling assumptions fail. All results are averaged over $10$ independent random runs. We test in both rank $1$ and rank $2$ settings to clearly illustrate the failures of the 

%In all our experiments user come uniform randomly over all time $[n]$. 

\subsection{Evaluated Algorithms}

\textbf{Rank $1$ Algorithms:} We compare against several state-of-the-art rank $1$ algorithms. Note, that all the rank $1$ algorithms suggest a single row and column at every round. The $\ucb$ algorithm from \citet{auer2002finite} builds a confidence set at every round $t$ over all the entries of $M_t$ as $c_{i, j}(t) = \sqrt{\frac{2\log t}{N_{i, j}(t)}}$ where $N_{i, j}(t)$ denotes the number of times the $M(i,j)$-th entry has been observed. It the suggests the best row-column pair based on the term $\hat{M}_{t}(i,j) + c_{i, j}(t)$ where $\hat{M}_{t}(i,j)$ denotes the empirical mean of all the observed rewards for $M(i,j)$. The $\ucbe$ \citep{auer2010ucb} is similar to $\ucb$ but it eliminates sub-optimal rows and columns  based on a similar confidence set  $c_{i, j}(t)$ till it finally converges on the best pair of row and column. The algorithm $\linucb$ was first proposed in \citet{li2010contextual} for the contextual bandit setting. Note, that for a set of features $\theta \in {0,1}^{K+L}$, rank-$1$ bandit generalizes to the stochastic linear bandit setting and can be solved by $\linucb$. Similarly, GLM-UCB from  \citet{filippi2010parametric} which computes the maximum-likelihood estimates of the parameter vector $\theta \in {0,1}^{K+L}$ (using Expectation-Maximization algorithm) can also be used solve the rank $1$ bandit problem. Finally, we compare against the algorithm $\rankelim$ from \citet{katariya2016stochastic} which is an improved version of $\ucbe$ and employs row and column elimination and aggressive exploration to converge on the best row and column pair. For $\latentranker$ we use the Algorithm \ref{alg:LRB} from Section \ref{sec:algorithm}. 
%We use \expthree as the \rowalg and \colalg with the exploration parameter $\gamma_{\text{\rowalg}} = \sqrt{\frac{K\log K}{T}}$ and $\gamma_{\text{\colalg}} = \sqrt{\frac{L\log L}{T}}$ respectively.


\textbf{Rank $2$ Algorithms:} We similarly design the Rank $2$ algorithms by modifying the rank $1$ algorithms. Again, note that all the rank $2$ algorithms suggest two pairs of rows and columns at every round $t$. For all of the algorithms $\ucb$, $\ucbe$, $\linucb$, $\glmucb$, and $\rankelim$ we modify these algorithms so that they suggest $2$ pairs of rows and columns based on their respective confidence interval set $c_{i, j}(t)$. The row and column pair with the highest and the second highest $\hat{M}_{t}(i,j) + c_{i, j}(t)$ are suggested for each round $t$ and consequently after observing all the entries of $M_t(i,j)$ all of the algorithms update their estimates of $\hat{M}_{t}(i,j)$ for each $i,j \in [d]$. For $\latentranker$ we use the Algorithm \ref{alg:LRB1} from Section \ref{sec:algorithm}. Note, that there are two $\rowalg$ and $\colalg$, each running an $\expthree$ algorithm with the  exploration parameters as discussed before.

\subsection{Synthetic Experiment $1$}
This experiment is conducted to test the performance of $\latentranker$ over a small number of users and items and to show how $\atentranker$ scales with increasing number of users and items. Note, that in this experiment all our modeling assumptions hold. This simulated testbed consist of two scenarios: (1) $8$ users and $8$ items and (2 )$16$ users and $16$ items. In this setting, $U_t = \{0.7, 0.9\}^{K\times 1}$ and similarly $V_t = \{0.7, 0.9\}^{L\times 1}$/ Hence, the matrix $M = UV^{\intercal}$ is rank $1$ and the hott-topics structure is maintained. At every round $t$, $u_t = \mathcal{D} + \Delta_u$ and $v_t =  \mathcal{D}_t  + \Delta_v$, where $\mathcal{D}_t \in \{0,1\}$ is independent Bernoulli noise and $\Delta_u = \Delta_v = 0.2$. The learner observes the entry $u_t(i)v_t(j)$ when it selects the $i$-th user and $j$-th item. From Figure \ref{fig:2} we can clearly see that $\latentranker$ outperforms all the other algorithms. The regret curve of $\latentranker$ flattens, indicating that it has learned the best user-item pair. As we scale the number of users and items we see that $\latentranker$ performs even better than other algorithms. The key realization is that $\latenranker$ takes advantage of the hott-topics structure and quickly identifies them. Note, that for any rank $d$ scenario the best user-item pair must be one of the hott-topics in $(I^*, J^*)$.


%Independent user model algorithms $\RBAUCB$ and $\RBAEXP$  perform poorly as the number of items per user is too large and the independent algorithms are not sharing information between them. $\NMFBan$ performs better than the independent user model algorithms but is outperformed by $\LRAEXP$, $\LRATS$ and $\LRAUCB$.


%The vectors spanning $U$ and $V$, generating the user-item preference matrix $M$, are shown Figure \ref{fig:1}. The users are evenly distributed into a $50:50$ split such that $50\%$ of users prefer item $1$ and $50\%$ users prefer item  $2$. The item hott-topics are $V(1,:) = (0,1)$ and $V(2,:) = (1, 0)$ while remaining $70\%$ of items has feature $V(j',:) = (0.45, 0.55)$ and the rest have $V(j,:) = (0.55, 0.45)$. We create the user feature matrix $U$ similarly having a $50:50$ split such that $U(1,:) = (0,1)$, $U(2,:) = (0.2,0.8)$ and the remaining $70\%$ users having $U(i,:) = (0,0.8)$ and $30\%$ users having $U(i',:) = (0.7,0)$. At every timestep $t$ the resulting matrix $M_t =UD_tV^{\intercal}$ is generated where $D_t$ is a randomly-generated diagonal matrix. So, $M_t$ is  such that algorithms that quickly find the easily identifiable hott-topics perform very well. From Figure \ref{fig:2} we can clearly see that $\LRAEXP$, $\LRATS$ and $\LRAUCB$ outperforms all the other algorithms. Their regret curve flattens, indicating that they have learned the best items for each user. Independent user model algorithms $\RBAUCB$ and $\RBAEXP$  perform poorly as the number of items per user is too large and the independent algorithms are not sharing information between them. $\NMFBan$ performs better than the independent user model algorithms but is outperformed by $\LRAEXP$, $\LRATS$ and $\LRAUCB$.

\subsection{Synthetic Experiment $2$}
This experiment is conducted to test the performance of $\latentranker$ over a large number of users and items. This simulated testbed consist of $64$ users, $64$ items, and rank$(M) = 2$. The vectors spanning $U$ and $V$, generating the user-item preference matrix $M$, are shown Figure \ref{fig:3}. The users and items are evenly distributed into a $50:50$ split such that $50\%$ of users prefer item $1$ and $50\%$ users prefer item $2$. The item hott-topics are $V(1,:) = (1,0)$ and $V(2,:) = (0, 0.6)$ while $50\%$ remaining  items has feature $V(j',:) = (0.45, 0.5)$ and the rest have $V(j,:) = (0.5, 0.45)$. Similarly, we create the user feature matrix $U$ having a $50:50$ split such that $U(1,:) = (1,0)$, $U(2,:) = (0,0.6)$ and the remaining $50\%$ users having $U(i,:) = (0.5,0.4)$ and the rest having $U(i',:) = (0.4,0.5)$. At every timestep $t$ the resulting matrix $M_t =UD_tV^{\intercal}$ is generated where $D_t$ is a randomly-generated diagonal matrix. So, $M_t$ is such that algorithms that quickly find the easily identifiable hott-topics perform very well. From Figure \ref{fig:2} we can clearly see that $\latentranker$ outperforms all the other algorithms. It's  regret curve flattens, indicating that it has learned the best user-item pair. 

%Independent user model algorithms $\RBAUCB$ and $\RBAEXP$  perform poorly as the number of items per user is too large and the independent algorithms are not sharing information between them. $\NMFBan$ performs better than the independent user model algorithms but is outperformed by $\LRAEXP$, $\LRATS$ and $\LRAUCB$.



\begin{figure}[!th]
\centering
\begin{tabular}{cc}
\setlength{\tabcolsep}{0.1pt}
\subfigure[0.25\textwidth][Expt-$1$: $500$ Users, $50$ items, Rank $2$, User and Item vectors]
    %with $r_{i_{{i}\neq {*}}}=0.07$ and $r^{*}=0.1$
    {
    		\includegraphics[scale=0.13]{img/Figure_8.png}
  		\label{fig:1}
    }
    &
    \subfigure[0.25\textwidth][Expt-$1$: Cumulative regret of different algorithms]
    %with $r_{i_{{i}\neq {*}}}=0.07$ and $r^{*}=0.1$
    {
    		\includegraphics[scale=0.13]{img/Figure_16.png}
  		\label{fig:2}
    }
    \\
    \subfigure[0.25\textwidth][Expt-$2$: $1500$ Users, $100$ items, Rank $3$, User and Item vectors]
    %with $r_{i_{{i}\neq {*}}}=0.07$ and $r^{*}=0.1$
    {
    		\includegraphics[scale=0.08]{img/rank_21_vec.png}
  		\label{fig:3}
    }
    &
    \subfigure[0.25\textwidth][Expt-$2$: $1500$ Users, $100$ items, Rank $3$, User and Item vectors]
    %with $r_{i_{{i}\neq {*}}}=0.07$ and $r^{*}=0.1$
    {
    		\includegraphics[scale=0.13]{img/Figure_64.png}
  		\label{fig:4}
    }
    \end{tabular}
    \caption{A comparison of the cumulative regret incurred by the various bandit algorithms. }
    \label{fig:karmed1}
    \vspace*{-1em}
\end{figure}


\subsection{Real World Experiment $3$}
We conduct the third experiment to test the performance of \latentranker when our modeling assumptions are violated. We use the Jester dataset \citep{goldberg2001eigentaste} which consist of over 4.1 million continuous ratings of 100 jokes from 73,421 users collected over 5 years. In this dataset there are many users who rated all jokes and we work with these users. Hence the user-item preference matrix is fully observed and we will not have to complete it using matrix completion techniques. Hence, this approach is very real world. We sample randomly $10$ users (who have rated all jokes) from this dataset and use singular value decomposition (SVD) to obtain a rank $2$ approximation of this user-joke rating matrix $M$. In the resultant matrix $M$, most of the users belong to the two classes preferring jokes $98$, and $28$, while a very small percentage of users prefer some other jokes. Note, that this condition results from the fact that this real-life dataset does not have the hott-topics structure. Furthermore, in this experiment we assume that the noise is independent Bernoulli over the entries of $M$ and hence this experiment deviates from our modeling assumptions. From \ref{fig:6} again we see that $\latentranker$ outperform other algorithms. 
\begin{wrapfigure}{l}{0.2\textwidth}
 \subfigure[0.2\textwidth][Expt-$1$: $500$ Users, $50$ items, Rank $2$, User and Item vectors]
    %with $r_{i_{{i}\neq {*}}}=0.07$ and $r^{*}=0.1$
    {
    		\includegraphics[scale=0.13]{img/Figure_J_1.png}
  		\label{fig:6}
    }
\end{wrapfigure}

%The rank $2$ approximation of $M$ of  is shown in Figure \ref{fig:5}, where we can clearly see the red stripes spanning the matrix indicating the low-rank structure of $M$. 
%\begin{figure}[!th]
%\centering
%\begin{tabular}{cc}
%\setlength{\tabcolsep}{0.1pt}
%\subfigure[0.25\textwidth][Expt-$1$: $500$ Users, $50$ items, Rank $2$, User and Item vectors]
%    %with $r_{i_{{i}\neq {*}}}=0.07$ and $r^{*}=0.1$
%    {
%    		\includegraphics[scale=0.11]{img/rank2_vec.png}
%  		\label{fig:5}
%    }
%    &
%    \subfigure[0.25\textwidth][Expt-$1$: $500$ Users, $50$ items, Rank $2$, User and Item vectors]
%    %with $r_{i_{{i}\neq {*}}}=0.07$ and $r^{*}=0.1$
%    {
%    		\includegraphics[scale=0.11]{img/Figure_J.png}
%  		\label{fig:1}
%  		\label{fig:6}
%    }
%    \end{tabular}
%    \caption{A comparison of the cumulative regret incurred by the various bandit algorithms. }
%    \label{fig:karmed1}
%    \vspace*{-1em}
%\end{figure}