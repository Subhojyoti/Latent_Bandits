%!TEX root = bandit_paper.tex

%\todob{Add a sentence or two that say what this section is about.}

%\todob{I did not double check the analysis. Let me know when I can.}

In this section we analyze the rank-$1$ $\latentranker$ and show its regret for a horizon $n$.
\begin{theorem} 
\label{thm:upper bound} Let $\colalg$ and $\rowalg$ in $\latentranker$ be $\expthree$ algorithm, respectively. Then the expected $n$-step regret of $\latentranker$ is bounded as
\begin{align*}
  R(n) = O\left(\frac{\left(\sqrt{L } + \sqrt{K }\right)\sqrt{n}}{\alpha}\right)
\end{align*}
for any $\alpha > 0$ such that $U_{t}(i,1) \geq \alpha$ and $V_{t}(j,1) \geq \alpha$  for all $i \in [K]$, $j \in [L]$, and $t \in [n]$.
%where $\alpha = \min_{t \in [n]} \min_{i_t,j_t:\ , i_t \neq i^*_t \, j_t \neq j^*_t} \E\left[u_t(i^*_t)v_t(j^*_t)\right] - \E\left[u_t(i_t)v_t(j_t)\right]$ \todob{I do not think that this definition of $\Delta$ is correct. If you look into the proof, you will note that $\Delta$ there has nothing to do with the gap. We should not call this quantity $\Delta$ because this will confuse reviewers.} is an instance-specific lower bound on the gap in the expected rewards of the optimal and best suboptimal columns and rows at any time $t \in [n]$, averaged over all users at that time.  

%\textbf{(Rank-1 Case)} \todob{No need to rub it in that this is rank $1$. This is the only theorem in the paper, right?}
%\todoan{Both should be $\geq$}
%\todob{We need to define the unweighted reward.}

\end{theorem}
\begin{proof}
Let, $(U_t V_t\transpose)_{t = 1}^n$ be a sequence of $n$ non-negative rank-$1$  matrices such that $U_t \in [0, 1]^{K \times 1}, V_t \in [0, 1]^{L \times 1}$, and the highest entry, without loss of generality, is $U_t(1) V_t(1)$. Let, $((i_t, j_t))_{t = 1}^n$
%\begin{align*}
%((i_t, j_t))_{t = 1}^n
%\end{align*}
be a sequence of $n$ row-column pairs chosen by a learning agent. Then the expected $n$-step regret is,
%\todob{All $U$ and $V$ should be capitalized, no? At least consistent with the notation in the setting.} 
%\todob{Properly write numbers and symbols. This should be ``$n$ non-negative rank-$1$''.}
%\todob{Properly bracket expectations. Lack of brackets repeats throughout the proof.}
\begin{align*}
R(n) = \sum_{t = 1}^n \E \left[{U_t(1) V_t(1) - U_t(i_t) V_t(j_t)}\right]
\end{align*}
where the expectation is over the randomness of the agent. For any two vectors $U$, $V$ and indices $i$, $j$ we have,
\begin{align*}
& 2 (U(1) V(1) - U(i) V(j)) \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%
 & =   2 U(1) V(1)  -  U(i) V(1) -  U(1) V(j) + \\
&     U(i) V(1) + U(1) V(j)  -  2 U(i) V(j) \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%
& = U(1) (V(1) - V(j))  + V(1) (U(1) -  U(i))  +\\
&  U(i) (V(1)  - V(j))  + V(j) (U(1)  - U(i)) \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%
& = \left(U(1) \!+\! U(i)\right) \left(V(1) \! - \! V(j)\right) \! +\! \left(V(1) \! + \! V(j)\right) \left(U(1) \! - \! U(i)\right)
\end{align*}
Therefore, the expected $n$-step regret can be decomposed as,
\begin{align*}
R(n) &= \sum_{t = 1}^n \E[{(V_t(1) + V_t(j_t)) (U_t(1) - U_t(i_t))}] \\
&+ \sum_{t = 1}^n \E[{(U_t(1) \! + \! U_t(i_t)) (V_t(1) - V_t(j_t))}]
\end{align*}
Now suppose that all entries of $U_t$ and $V_t$ for all $t=1,2,\ldots, n$ are bounded from below by some $\alpha > 0$. Then we get that,
\begin{align*}
R(n)
& = \sum_{t = 1}^n \E[{(1 + V_t(1) / V_t(j_t)) V_t(j_t) (U_t(1) - U_t(i_t))}] +\\
&\sum_{t = 1}^n \E[{(1 + U_t(1) / U_t(i_t)) U_t(i_t) (V_t(1) - V_t(j_t))}] \\
& \leq (1 + \frac{1}{\alpha}) \bigg[\sum_{t = 1}^n \E[{U_t(1) V_t(j_t) - U_t(i_t) V_t(j_t)}] + \\
& \sum_{t = 1}^n \E[{U_t(i_t) V_t(1) - U_t(i_t) V_t(j_t)}]\bigg]
\end{align*}
%\todob{The argument below is just bla bla bla and needs to be written properly. In particular, it needs to be clear what we condition on and what we take the expectation over.}
The $\colalg$ chooses the column $j_t$ at round $t$ and observes reward $U_t(i_t) V_t(j_t)$. Note that from eq \eqref{eq:rank1ij} we know that the best row $i^*$ and best column $j^*$ is fixed for all $t\in [n]$. Hence, we can show that the $\colalg$ using $\expthree$ will converge on $j^*$ for any sequence of rows. Therefore, the first sum above is bounded by $\sqrt{L n}$ for any sequence of $i_t$, and thus also in expectation over the randomness in $i_t$. Similarly $\rowalg$ using $\expthree$ chooses the row $i_t$ at round $t$, and observe reward is $U_t(i_t) V_t(j_t)$. Following eq \eqref{eq:rank1ij} we can again show that $\rowalg$ will converge on $i^*$ for any sequence of columns. Therefore, the second sum above is bounded by $\sqrt{K n}$ for any sequence of $j_t$, and thus also in expectation over the randomness in $j_t$. Therefore we get the final regret as,
\begin{align*}
  R(n) = O\bigg(\frac{(\sqrt{L } + \sqrt{K })\sqrt{n}}{\alpha}\bigg)
\end{align*}
%\todob{$n$ above should be $\sqrt{n}$, right?}
\end{proof}
%\todob{Write discussion as text. It looks weird when large parts of the paper are in italic.}
%\todob{What probability? There is none. We bound the expected regret.}
%\todob{What probability? We bound the expected regret due to learning the optimal column, for any sequence of rows. This goes back to the proof, which is written bla bla bla.}
%\todob{What probability? We bound the expected regret due to learning the optimal row, for any sequence of columns.}
%\begin{discussion}
\textbf{Discussion:} The main idea in Theorem \ref{thm:upper bound} is to decompose the regret of $\latentranker$ into two parts, where $\colalg$ does not suggest $j^*$ and the $\rowalg$ does not suggest $i^*$. The first part is analyzed as follows. $\colalg$ has a sub-linear regret, as we use $\expthree$ as the $\colalg$. Thus,  the expected regret that $\colalg$ suggests sub-optimal columns for \emph{any sequence} of rows is bounded. This regret scales as $O(\sqrt{L n})$ based on the analysis of \citet{auer2002nonstochastic}. Similarly, we analyze the regret for the $\rowalg$ as it also uses the exponentially weighted algorithm $\expthree$. The $\rowalg$ suffers a regret of $O(\sqrt{K n})$ for suggesting sub-optimal rows for \emph{any sequence} of columns. Hence, combining both the parts we get the regret of order $O\left(\frac{\left(\sqrt{L } + \sqrt{K }\right)\sqrt{n}}{\alpha}\right)$ in \cref{thm:upper bound}. We use non-stochastic algorithm $\expthree$ for $\colalg$ and $\rowalg$ because our environment is non-stationary. Finally, we can consider any matrix $M$ of dimension $K\times L$ as a $K$-bandit problem with each bandit having $L$ arms and the optimal columns are learned separately for each bandit. Such a trivial setting gives rise to the regret bound of order $O(\sqrt{K L n})$. The regret bound for our algorithm is much better than this for certain parameter ranges of the problem.

%is due to learning the optimal column and the second part, which is $O(\sqrt{K n})$, is the regret due to learning the optimal row.
%consists of two main parts. \todob{We keep repeating ourselves. You already said that the regret decomposes in two parts and that each part can be bounded independently.} The first part, which is $O(\sqrt{L n})$,
% based on a similar analysis to \citet{auer2002nonstochastic}. \todob{Why do you cite this paper? Our analysis has nothing to do with what Auer did in 2002.}
%and increases \todob{Decreases with $n$? $\sqrt{n}$ does not decrease with $n$.} with time horizon $n$
%\todoan{The optimal row,columns should be $i^*,j^*$, shouldn't have a time subscript.}
% where the optimal columns are learned separately for each user. In that case, the regret would be $O(\sqrt{K L n})$. 

%\todob{How does this go back to RBA? This is a gap-free upper bound on the regret of any algorithm that would treat each entry as a separate arm. In fact, there is a matching lower bound in this setting, right?}

%If this problem was solved by RBA \citep{radlinski2008learning}

%Finally, we use non-stochastic algorithms for $\colalg$ and $\rowalg$ because our environment is non-stationary. In particular, we assume that user preferences $U_t$, and thus rewards, can change over time $t$. 
%In addition, the rewards in $\colalg(k)$ are non-stationary due to chosen columns at higher positions $1, \dots, k - 1$.
%\end{discussion}