In this work, we study the problem of learning the maximum entry of a low-rank matrix from sequential observations. These type of low-rank structure is observed in many real-world applications and is a standard assumption in recommender systems \citep{koren2009matrix,ricci2011liorrokach}. Our learning model is motivated by a real-world scenario, where a marketer wants to advertise a product and has $K$ population segments and $L$ marketing channels. Now, given a product some population segment prefer some marketing channels more than other. Hence, a successful conversion happens if each population segment is matched to the correct marketing channel which is nothing but the maximum entry of the matrix formed by the outer product of the users preference and marketing channels over some number of common topics.  

We formalize our learning problem as the following online learning problem. At time $t$, the learning agent chooses $d$-pairs of rows and columns where $d$ is the rank of a non-negative and low-rank matrix $M$. We use the terminology row/user and column/item interchangeably, keeping in sync with our proposed application area . This user-item preference matrix $M$ is formed by the outer product of user and item latent preferences over $d$ topics. Note that the learner does not observe the individual latent values of user or item preferences but just their product. The user-item preference matrix $M$ is low-rank at each round $t$, can vary substantially over time, and does not have to be stochastic. The goal of our learning agent is to minimize the cumulative regret with respect to a best solution in hindsight by finding the maximum entry in $M$ as quickly as possible.



Previous works that have studied this setting have either proposed highly conservative algorithms or restricted themselves to a stricter set of assumptions. While \citet{katariya2016stochastic} was proposed for a rank $1$ bandit model with the assumption that the underlying distributions are stochastic, \citet{katariya2017bernoulli} was proposed for the special case when the underlying distributions are Bernoulli. Both these works used different variations of the phase-based UCB-Improved \citep{auer2010ucb} algorithm to construct a confidence interval set over row-column pairs to identify and eliminate sub-optimal rows and columns. These naturally results in algorithms that explore conservatively (for the sake of row and column elimination) and cannot work beyond the stochastic distribution assumption. Finally, \citet{kveton2017stochastic} can be viewed as a generalization of rank-$1$ bandits of \citet{katariya2016stochastic} to a higher rank of $d$. However, this work proposes a phase-based algorithm that calculates the square of the determinant of a $d\times d$ sub-matrix to eliminate sub-optimal rows and columns at the end of phases which is impractical for very large non-negative low-rank matrices. Some other approaches involving non-negative matrix factorization \citet{sen2016contextual} or tensor based methods \citep{gopalan2016low} to reconstruct the matrix have also been proposed. These works require strong assumptions on the structure of the matrix such as all the matrices satisfy a weak statistical Restricted Isometric Property (RIP) or calculate third order tensors as in \citet{anandkumar2014tensor}. A more simpler setting has also been studied in \citet{maillard2014latent}.

%are detailed in Section \ref{sec:related work}
 %\citet{sen2016contextual} is an online matrix completion algorithm which is an $\epsilon$-greedy algorithm that tries to reconstruct the matrix $M$ through non-negative matrix factorization. Note, that this approach requires that all the matrices satisfy a weak statistical Restricted Isometric Property, which is not always feasible in real life applications. Another approach is that of \citet{gopalan2016low} where the authors come up with an algorithm which uses the Robust Tensor Power (RTP) method of 
%\citet{anandkumar2014tensor} to reconstruct the matrix $M$, and then use the OFUL procedure of \citet{abbasi2011improved} to behave greedily over the reconstructed matrix. 
%But the RTP is a costly operation because the learner needs to construct a matrix of order $L\times L$ and $L\times L \times L$ to calculate the second and third order tensors for the reconstruction.  A more simpler setting has also been studied in \citet{maillard2014latent}

Our approach is based on two key insights. First, the earlier methods (like Upper Confidence Bound (UCB) algorithms, NMF-Bandits \citep{sen2016contextual}) are explicitly modeled on the stochastic i.i.d assumption on feedback and cannot perform well in non-stochastic settings. Moreover, their theoretical guarantees will also fail in non-stochastic setting. Hence, we need algorithms that can work on more generalized non-stochastic probability distribution settings. Secondly, we can formulate simple and computationally efficient algorithms that learn the best set of columns and best set of rows jointly with two separate non-stochastic bandit algorithm operating on rows and columns individually. These does not require any sort of costly matrix inversion or reconstruction operations or even row or column eliminations and hence are faster in implementation. 

%We formalize our learning problem as the following online learning problem. At time $t$, a random user $i_t$ from a pool of $K$ users arrives to the recommender system. The learning agent observes the identity of the user $i_t$, recommends a list of $d$ diverse items $J_t$ from a pool of $L$ items as a response, and observes the preferences of user $i_t$ for all recommended items $J_t$. The user-item preference matrix is low-rank at each time $t$, can vary substantially over time, and does not have to be stochastic. The reward of the recommended list is high when highly preferred items of the user are recommended at higher positions. The goal of our learning agent is to compete with the most rewarding diverse list for each user in hindsight.

%Our learning model is motivated by a real-world scenario, where the learning agent suggests movies to users and each movie belongs to different movie genres. The agent typically does not observe instantaneous preferences of the user, and therefore suggests multiple movies that may be of interest to the user under different circumstances. A similar model has also been studied in \citet{carbonell1998use} where the goal is to suggest a diversified list to each incoming user that combines relevance to the query as well as novelty. The authors suggest an approach where each item in the list is relevant to the query but also has \textit{"marginal relevance"} or less similarity with previously selected documents and this improves the quality of recommendation.

%The key structure of our matrix is that the set of optimal items of all users is small and can be learned jointly for all users. Given these items, the problem of learning the optimal order for each user can be solved in the full-information setting and thus is easy. 

%The column learning algorithm is similar to ranked bandits. In particular, we learn the $k$-th most diverse item using a multi-armed bandit, whose rewards are conditioned on the rewards of $k - 1$ previously chosen items. The row learning problem is solved separately for each user. Because it is in the full-information setting, as we observe the individual rewards of all recommended items,  we solve it using the weighted majority algorithm.

We make four major contributions. First, we formulate our online learning problem as a non-stochastic bandit problem on a class of non-negative low-rank matrices. We identify a family of non-negative low-rank matrices where our problem can be solved statistically efficiently, without actually observing the latent values of individual rows and columns. Second, we propose a computationally-efficient algorithm that implements this idea, which we call Low Rank Bandit ($\latentranker$) algorithm. The algorithm has two components, column learning and row learning, which learn the pair  of optimal columns and rows respectively. Since we are in the non-stochastic setting we use a variation of the $\expthree$ \citep{auer2002nonstochastic} algorithm as our row and column learner. Note, that we do not construct any confidence interval or eliminate rows and columns like the existing works. Infact, we use the well known fact that exponentially weighted algorithm like $\expthree$ are robust and fast learner to design our algorithm. The Third, we analyze $\latentranker$ and up to problem-specific factors, we prove a $O\left(\frac{\left(\sqrt{L } + \sqrt{K }\right)\sqrt{n}}{\Delta}\right)$ upper bound on its $n$-step regret in the special case when rank is $1$. The regret of a naive solution is $O(\sqrt{K L n})$, and is much worse than that of $\latentranker$ when all of $K$, $L$, and $n$ are large. Finally, we evaluate $\latentranker$ empirically on several synthetic and real-world problems. Perhaps surprisingly, $\latentranker$ performs well even when our modeling assumptions are violated.

The paper is organized as follows. We introduce necessary background and settings to understand our work for the rank-$1$ scenario in \cref{sec:background}. We then introduce the rank-$1$ algorithm in \cref{sec:rank1} and bound the regret of this rank-$1$ algorithm in \cref{sec:analysis}. We then analyze the general rank $d$ setting in \cref{sec:setting} and propose our algorithm for the general rank $d$ in \cref{sec:algorithm} . In \cref{sec:experiments}, we evaluate the algorithm empirically. We conclude in \cref{sec:conclusions}. 

%and define our online learning problem in \cref{sec:setting}
%The detailed proof of our regret bound is presented in \cref{sec:proof}.
% In \cref{sec:related work}, we survey related work.