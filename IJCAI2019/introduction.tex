In this work, we study the problem of learning the maximum entry of a low-rank matrix from sequential observations. The low-rank structure is observed in many real-world applications and is a standard assumption in recommender systems \citep{koren2009matrix,ricci2011liorrokach}. Our learning model is motivated by a real-world scenario, where a marketer wants to advertise a product, and has $K$ users and $L$ marketing channels. The marketer can choose pairs of users and marketing channels to promote its product. Let the users and marketing channels be the rows and columns of a low rank matrix. Then the goal of the learner is to maximize its click-through rate (CTR) by finding the maximum entry of this matrix.

%the low-rank matrix formed by the outer product of the users preference and marketing channels over some number of common topics.


%Now, given a product some population segment prefer some marketing channels more than other. \todob{Give a plain English example of what we mean by this. Potentially cite.} Hence, a successful conversion happens if each population segment is matched to the correct marketing channel which is nothing but the maximum entry of the matrix formed by the outer product of the users preference and marketing channels over some number of common topics.  \todoan{This is misleading. We don't match each population segment with a marketing channel, but just identify the pair with maximum reward.}

We formalize our learning problem as the following online learning problem. At round $t$, the learning agent chooses $d$-tuples of rows and columns where $d$ is the rank of a non-negative and low-rank matrix $M$. We use the terminology row/user and column/item interchangeably, keeping in sync with our proposed application area. This matrix $M$ is formed by the outer product of row and column latent preferences over $d$ topics. Hence, $M$ encodes the preference of row over items and is termed as row-column preference matrix. Note that the learner does not observe the individual latent vectors of row or column preferences but just their product. The row-column preference matrix $M$ is low-rank at each round $t$, can vary substantially over time, and does not have to be stochastic. The goal of our learning agent is to minimize the cumulative regret with respect to a best solution in hindsight by finding the maximum entry in $M$ as quickly as possible.
%\todob{Again, the term preference appears out of nowhere.}



%\todoan{Do you want to say the algorithms run faster or that they easier to implement?} 

%We formalize our learning problem as the following online learning problem. At time $t$, a random row $i_t$ from a pool of $K$ users arrives to the recommender system. The learning agent observes the identity of the row $i_t$, recommends a list of $d$ diverse items $J_t$ from a pool of $L$ items as a response, and observes the preferences of row $i_t$ for all recommended items $J_t$. The row-column preference matrix is low-rank at each time $t$, can vary substantially over time, and does not have to be stochastic. The reward of the recommended list is high when highly preferred items of the row are recommended at higher positions. The goal of our learning agent is to compete with the most rewarding diverse list for each row in hindsight.

%Our learning model is motivated by a real-world scenario, where the learning agent suggests movies to users and each movie belongs to different movie genres. The agent typically does not observe instantaneous preferences of the row, and therefore suggests multiple movies that may be of interest to the row under different circumstances. A similar model has also been studied in \citet{carbonell1998use} where the goal is to suggest a diversified list to each incoming row that combines relevance to the query as well as novelty. The authors suggest an approach where each column in the list is relevant to the query but also has \textit{"marginal relevance"} or less similarity with previously selected documents and this improves the quality of recommendation.

%The key structure of our matrix is that the set of optimal items of all users is small and can be learned jointly for all users. Given these items, the problem of learning the optimal order for each row can be solved in the full-information setting and thus is easy. 

%The column learning algorithm is similar to ranked bandits. In particular, we learn the $k$-th most diverse column using a multi-armed bandit, whose rewards are conditioned on the rewards of $k - 1$ previously chosen items. The row learning problem is solved separately for each row. Because it is in the full-information setting, as we observe the individual rewards of all recommended items,  we solve it using the weighted majority algorithm.

We make four major contributions. First, we formulate our online learning problem as a non-stochastic bandit problem on a class of non-negative low-rank matrices. We identify a family of non-negative low-rank matrices where our problem can be solved statistically efficiently, without actually observing the latent values of individual rows and columns. Second, we propose a computationally-efficient algorithm that implements this idea, which we call a low-rank bandit algorithm ($\latentranker$). The algorithm has two components, column learning and row learning, which learn the pair  of optimal columns and rows, respectively. Since we are in the non-stochastic setting, we use a variation of $\expthree$ \citep{auer2002nonstochastic} as our row and column learner. Note that we do not construct any confidence interval or eliminate rows and columns like the existing works. In fact, we use the well known fact that exponentially-weighted algorithms, like $\expthree$, are robust and fast learners to design our algorithm. Third, we analyze $\latentranker$ and up to problem-specific factors, we prove a $O\left(\frac{\left(\sqrt{L } + \sqrt{K }\right)\sqrt{n}}{\alpha}\right)$ upper bound on its $n$-step regret in the special case when rank is $1$ and for some $\alpha > 0$. The regret of a naive solution is $O(\sqrt{K L n})$, and is much worse than that of $\latentranker$ when $K \approx L$. Finally, we evaluate $\latentranker$ empirically on several synthetic and real-world problems. Perhaps surprisingly, $\latentranker$ performs well even when our modeling assumptions are violated.
%\todoan{I don't think we should call these as four major contributions. For example, if we propose an algorithm in a paper, it better work. We don't say we make two major contributions in that case, proposing an algorithm and showing it is true.}
%\todob{You keep calling our algorithm both ``Low Rank Bandit'' and $\latentranker$ in the same sentence. This is super confusing. The point of the abbreviation is that you do not need to repeat the full name. Stick to $\latentranker$ and explain here that this stands for low rank bandit. Note that I did not use CAPS.} 

The paper is organized as follows. In \cref{sec:background}, we introduce the rank-$1$ setting. In \cref{sec:rank1,sec:analysis}, we propose our rank-$1$ algorithm and derive a sublinear upper bound on its regret. In \cref{sec:setting,sec:algorithm}, we introduce the rank-$d$ setting and propose an algorithm for it. In \cref{sec:experiments}, we evaluate our algorithms empirically. We discuss related works in \cref{sec:related} and conclude in \cref{sec:conclusions}. 
