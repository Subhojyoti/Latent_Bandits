In this work, we study the problem of learning the maximum entry of a low-rank matrix from sequential observations. The low-rank structure is observed in many real-world applications and is a standard assumption in recommender systems \citep{koren2009matrix,ricci2011liorrokach}. Our learning model is motivated by a real-world scenario, where a marketer wants to advertise a product and has $K$ population segments and $L$ marketing channels. Now, given a product some population segment prefer some marketing channels more than other. \todob{Give a plain English example of what we mean by this. Potentially cite.} Hence, a successful conversion happens if each population segment is matched to the correct marketing channel which is nothing but the maximum entry of the matrix formed by the outer product of the users preference and marketing channels over some number of common topics.  \todoan{This is misleading. We don't match each population segment with a marketing channel, but just identify the pair with maximum reward.}

We formalize our learning problem as the following online learning problem. At time $t$, the learning agent chooses $d$-pairs \todob{A pair means $2$. If you have more than $2$, it is a tuple.} of rows and columns where $d$ is the rank of a non-negative and low-rank matrix $M$. We use the terminology row/user and column/item interchangeably, keeping in sync with our proposed application area. This user-item preference \todob{Again, the term preference appears out of nowhere.} matrix $M$ is formed by the outer product of user and item latent preferences over $d$ topics. Note that the learner does not observe the individual latent values \todob{latent vectors} of user or item preferences but just their product. The user-item preference matrix $M$ is low-rank at each round $t$, can vary substantially over time, and does not have to be stochastic. The goal of our learning agent is to minimize the cumulative regret with respect to a best solution in hindsight by finding the maximum entry in $M$ as quickly as possible.

\todob{The two paragraphs below should be moved to ``Related Work''. They contain too many details that are not necessary to understand our design and contributions. Put ``Related Work'' right before ``Conclusions''. Also, the current comparison to prior work is a laundry list and completely inefficient. Better structure how we differ. Paragraph 1: Some people do only rank $1$. We do rank $d$. Paragraph 2: Most papers do stochastic. We do adversarial (with restrictions). Paragraph 3: Our main selling point is a simple algorithm that can be easily generalized beyond rank $1$. Focus on this difference.}

Previous works that have studied this setting have either proposed highly conservative algorithms or restricted themselves to a stricter set of assumptions. While \citet{katariya2016stochastic} was proposed for a rank $1$ bandit model with the assumption that the underlying distributions are stochastic, \citet{katariya2017bernoulli} was proposed for the special case when the underlying distributions are Bernoulli. Both these works used different variations of the phase-based UCB-Improved \citep{auer2010ucb} algorithm to construct a confidence interval set over row-column pairs to identify and eliminate sub-optimal rows and columns. These naturally results in algorithms that explore conservatively (for the sake of row and column elimination) and cannot work beyond the stochastic distribution assumption. Finally, \citet{kveton2017stochastic} can be viewed as a generalization of rank-$1$ bandits of \citet{katariya2016stochastic} to a higher rank of $d$. However, this work proposes a phase-based algorithm that calculates the square of the determinant of a $d\times d$ sub-matrix to eliminate sub-optimal rows and columns at the end of phases which is impractical for very large non-negative low-rank matrices. Some other approaches involving non-negative matrix factorization \citet{sen2016contextual} or tensor based methods \citep{gopalan2016low} to reconstruct the matrix have also been proposed. These works require strong assumptions on the structure of the matrix such as all the matrices satisfy a weak statistical Restricted Isometric Property (RIP) or calculate third order tensors as in \citet{anandkumar2014tensor}. A more simpler setting has also been studied in \citet{maillard2014latent}.

%are detailed in Section \ref{sec:related work}
 %\citet{sen2016contextual} is an online matrix completion algorithm which is an $\epsilon$-greedy algorithm that tries to reconstruct the matrix $M$ through non-negative matrix factorization. Note, that this approach requires that all the matrices satisfy a weak statistical Restricted Isometric Property, which is not always feasible in real life applications. Another approach is that of \citet{gopalan2016low} where the authors come up with an algorithm which uses the Robust Tensor Power (RTP) method of 
%\citet{anandkumar2014tensor} to reconstruct the matrix $M$, and then use the OFUL procedure of \citet{abbasi2011improved} to behave greedily over the reconstructed matrix. 
%But the RTP is a costly operation because the learner needs to construct a matrix of order $L\times L$ and $L\times L \times L$ to calculate the second and third order tensors for the reconstruction.  A more simpler setting has also been studied in \citet{maillard2014latent}

Our approach is based on two key insights. \todoan{I wouldn't call these key insights. These are points for how we are different from earlier papers.}First, the earlier methods (like Upper Confidence Bound (UCB) algorithms, NMF-Bandits \citep{sen2016contextual}) are explicitly modeled on the stochastic i.i.d assumption on feedback and cannot perform well in non-stochastic settings. Moreover, their theoretical guarantees will also fail in non-stochastic setting. Hence, we need algorithms that can work on more generalized non-stochastic probability distribution settings. Secondly, we can formulate simple and computationally efficient algorithms that learn the best set of columns and best set of rows jointly with two separate non-stochastic bandit algorithm operating on rows and columns individually. These do not require any sort of costly matrix inversion or reconstruction operations or even row or column eliminations and hence are faster to implementation. \todoan{Do you want to say the algorithms run faster or that they easier to implement?} 

%We formalize our learning problem as the following online learning problem. At time $t$, a random user $i_t$ from a pool of $K$ users arrives to the recommender system. The learning agent observes the identity of the user $i_t$, recommends a list of $d$ diverse items $J_t$ from a pool of $L$ items as a response, and observes the preferences of user $i_t$ for all recommended items $J_t$. The user-item preference matrix is low-rank at each time $t$, can vary substantially over time, and does not have to be stochastic. The reward of the recommended list is high when highly preferred items of the user are recommended at higher positions. The goal of our learning agent is to compete with the most rewarding diverse list for each user in hindsight.

%Our learning model is motivated by a real-world scenario, where the learning agent suggests movies to users and each movie belongs to different movie genres. The agent typically does not observe instantaneous preferences of the user, and therefore suggests multiple movies that may be of interest to the user under different circumstances. A similar model has also been studied in \citet{carbonell1998use} where the goal is to suggest a diversified list to each incoming user that combines relevance to the query as well as novelty. The authors suggest an approach where each item in the list is relevant to the query but also has \textit{"marginal relevance"} or less similarity with previously selected documents and this improves the quality of recommendation.

%The key structure of our matrix is that the set of optimal items of all users is small and can be learned jointly for all users. Given these items, the problem of learning the optimal order for each user can be solved in the full-information setting and thus is easy. 

%The column learning algorithm is similar to ranked bandits. In particular, we learn the $k$-th most diverse item using a multi-armed bandit, whose rewards are conditioned on the rewards of $k - 1$ previously chosen items. The row learning problem is solved separately for each user. Because it is in the full-information setting, as we observe the individual rewards of all recommended items,  we solve it using the weighted majority algorithm.

We make four major contributions. \todoan{I don't think we should call these as four major contributions. For example, if we propose an algorithm in a paper, it better work. We don't say we make two major contributions in that case, proposing an algorithm and showing it is true.} First, we formulate our online learning problem as a non-stochastic bandit problem on a class of non-negative low-rank matrices. We identify a family of non-negative low-rank matrices where our problem can be solved statistically efficiently, without actually observing the latent values of individual rows and columns. Second, we propose a computationally-efficient algorithm that implements this idea, which we call Low Rank Bandit ($\latentranker$) algorithm. \todob{You keep calling our algorithm both ``Low Rank Bandit'' and $\latentranker$ in the same sentence. This is super confusing. The point of the abbreviation is that you do not need to repeat the full name. Stick to $\latentranker$ and explain here that this stands for low rank bandit. Note that I did not use CAPS.} The algorithm has two components, column learning and row learning, which learn the pair  of optimal columns and rows respectively. Since we are in the non-stochastic setting we use a variation \todoan{'variant'} of the $\expthree$ \citep{auer2002nonstochastic} algorithm as our row and column learner. Note, that we do not construct any confidence interval or eliminate rows and columns like the existing works. In fact, we use the well known fact that exponentially weighted algorithm like $\expthree$ are robust and fast learners to design our algorithm. The Third, we analyze $\latentranker$ and up to problem-specific factors, we prove a $O\left(\frac{\left(\sqrt{L } + \sqrt{K }\right)\sqrt{n}}{\alpha}\right)$ upper bound on its $n$-step regret in the special case when rank is $1$ and for some $\alpha > 0$. The regret of a naive solution is $O(\sqrt{K L n})$, and is much worse than that of $\latentranker$ when $K \approx L$. Finally, we evaluate $\latentranker$ empirically on several synthetic and real-world problems. Perhaps surprisingly, $\latentranker$ performs well even when our modeling assumptions are violated.

The paper is organized as follows. In \cref{sec:background}, we introduce the rank-$1$ setting. In \cref{sec:rank1,sec:analysis}, we propose our rank-$1$ algorithm and derive a sublinear upper bound on its regret. In \cref{sec:setting,sec:algorithm}, we introduce the rank-$d$ setting and propose an algorithm for it. In \cref{sec:experiments}, we evaluate our algorithms empirically. We conclude in \cref{sec:conclusions}. 
