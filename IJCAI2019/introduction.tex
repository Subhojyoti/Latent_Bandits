In this work, we study the problem of learning the maximum entry of a low-rank matrix from sequential observations. These type of low-rank structure is observed in many real-world applications and is a standard assumption in recommender systems \citep{koren2009matrix,ricci2011liorrokach}. Our learning model is motivated by a real-world scenario, where a marketer wants to advertise a product and has $K$ population segments and $L$ marketing channels. Now, given a product some population segment prefer some marketing channels more than other. Hence, a successful conversion happens if each population segment is matched to the correct marketing channels which is nothing but the maximum entry of the matrix formed by the outer product of the users preference for the product and marketing channels.  

We formalize our learning problem as the following online learning problem. At time $t$, the learning agent chooses $d$-pairs of rows and columns where $d$ is the rank of a non-negative and low-rank matrix $M$. This user-item preference matrix $M$ is formed by the outer product of user and item latent preference over $d$ topics. Note that the learner does not observe the individual latent values of user or item preference but just a noisy realization of their product. The user-item preference matrix $M$ is low-rank at each time $t$, can vary substantially over time, and does not have to be stochastic. The goal of our learning agent is to minimize the cumulative regret with respect to a best solution in hindsight by finding the maximum entry in $M$ as fast as possible.



Previous works that have studied this setting have either proposed highly conservative algorithms or restricted themselves to a stricter set of assumptions. While \citet{katariya2016stochastic} was proposed for a rank-1 bandit model with the assumption that the underlying distributions is stochastic, \citet{katariya2017bernoulli} was proposed for the special case when the underlying distribution is Bernoulli. Both these works used different variations of the phase-based UCB-Improved \citep{auer2010ucb} algorithm to construct confidence interval to identify and eliminate sub-optimal rows and columns. These naturally results in algorithms that explore conservatively (for the sake of row and column elimination) and cannot work beyond the stochastic distribution assumption. Finally, \citet{kveton2017stochastic} can be viewed as a generalization of rank-1 bandits of \citet{katariya2016stochastic} to a higher rank of $d$. However, this work proposes a phase-based algorithm that calculates the square of the determinant of a $d\times d$ sub-matrix to eliminate sub-optimal rows and columns at the end of phases which is impractical for very large non-negative low-rank matrices. Some other approaches involving non-negative matric factorization or tensor based methods are detailed in Section \ref{sec:related work}. These works also require strong assumptions on the structure of the matrix and involves costly matrix inversion or reconstruction operations.

Our approach is based on two key insights. First, the earlier methods (like Upper Confidence Bound (UCB) algorithms, NMF-Bandits) are explicitly modeled on the stochastic i.i.d assumption on feedback and cannot perform well in non-stochastic settings. Moreover, their theoretical guarantees will also fail in non-stochastic setting. Hence, we need algorithms that can work on more generalized non-stochastic probability distribution settings. Secondly, we can formulate simple and computationally efficient algorithms that learn the best set of columns and best set of rows jointly with two separate non-stochastic bandit algorithm operating on rows and columns individually. These does not require any sort of costly matrix inversion or reconstruction operations or even row or column eliminations and hence are faster in implementation. 

%We formalize our learning problem as the following online learning problem. At time $t$, a random user $i_t$ from a pool of $K$ users arrives to the recommender system. The learning agent observes the identity of the user $i_t$, recommends a list of $d$ diverse items $J_t$ from a pool of $L$ items as a response, and observes the preferences of user $i_t$ for all recommended items $J_t$. The user-item preference matrix is low-rank at each time $t$, can vary substantially over time, and does not have to be stochastic. The reward of the recommended list is high when highly preferred items of the user are recommended at higher positions. The goal of our learning agent is to compete with the most rewarding diverse list for each user in hindsight.

%Our learning model is motivated by a real-world scenario, where the learning agent suggests movies to users and each movie belongs to different movie genres. The agent typically does not observe instantaneous preferences of the user, and therefore suggests multiple movies that may be of interest to the user under different circumstances. A similar model has also been studied in \citet{carbonell1998use} where the goal is to suggest a diversified list to each incoming user that combines relevance to the query as well as novelty. The authors suggest an approach where each item in the list is relevant to the query but also has \textit{"marginal relevance"} or less similarity with previously selected documents and this improves the quality of recommendation.

%The key structure of our matrix is that the set of optimal items of all users is small and can be learned jointly for all users. Given these items, the problem of learning the optimal order for each user can be solved in the full-information setting and thus is easy. 

%The column learning algorithm is similar to ranked bandits. In particular, we learn the $k$-th most diverse item using a multi-armed bandit, whose rewards are conditioned on the rewards of $k - 1$ previously chosen items. The row learning problem is solved separately for each user. Because it is in the full-information setting, as we observe the individual rewards of all recommended items,  we solve it using the weighted majority algorithm.

We make four major contributions. First, we formulate our online learning problem as a non-stochastic bandit problem on a class of non-negative low-rank matrices. We identify a family of non-negative low-rank matrices where our problem can be solved statistically efficiently, without actually observing the latent values of individual rows and columns. Second, we propose a computationally-efficient algorithm that implements this idea, which we call Low Rank Bandit ($\latentranker$) algorithm. The algorithm has two components, column learning and row learning, which learn the pair  of optimal columns and rows respectively. Since we are in the non-stochastic setting we use a variation of the \expthree \citep{auer2002nonstochastic} algorithm as our row and column learner. Note, that we do not construct any confidence interval or eliminate rows and columns like the existing works. Infact, we use the well known fact that exponentially weighted algorithm like \expthree are robust and fast learner to construct our algorithm. The Third, we analyze $\latentranker$ and up to problem-specific factors, we prove a $O\left(\frac{\left(\sqrt{L } + \sqrt{K }\right)n}{\Delta}\right)$ upper bound on its $n$-step regret in the special case when rank is $1$. The regret of a naive solution is $O(\sqrt{K L n})$, and is much worse than that of $\latentranker$ when all of $K$, $L$, and $n$ are large. Finally, we evaluate $\latentranker$ empirically on several synthetic and real-world problems. Perhaps surprisingly, $\latentranker$ performs well even when our modeling assumptions are violated.

The paper is organized as follows. We introduce necessary background to understand our work in \cref{sec:background} and define our online learning problem in \cref{sec:setting}. We propose our algorithm in \cref{sec:algorithm} and bound its regret in \cref{sec:analysis}. In \cref{sec:experiments}, we evaluate the algorithm empirically. In \cref{sec:related work}, we survey related work. We conclude in \cref{sec:conclusions}. 

%The detailed proof of our regret bound is presented in \cref{sec:proof}.
