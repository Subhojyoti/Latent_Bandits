Previous works that have studied this setting have focused either on the rank-$1$ setting or have proposed solution where the underlying distributions are stochastic and having some structure.

\textbf{Rank-$1$ Setting:} The work of \citet{katariya2016stochastic} was proposed for a rank-$1$ bandit model with the assumption that the underlying distributions are stochastic. Similarly, \citet{katariya2017bernoulli} was proposed for the special case when the underlying distributions are Bernoulli.  A more simpler setting has also been studied in \citet{maillard2014latent}. All of these works used different variations of the Upper Confidence Bound (UCB) algorithm \cite{auer2002finite}, \citep{auer2010ucb} algorithm to construct a confidence interval set over row-column pairs to identify and eliminate sub-optimal rows and columns. These naturally results in algorithms that explore conservatively (for the sake of row and column elimination) and cannot work beyond the stochastic distribution assumption. 

%A more simpler setting has also been studied in \citet{maillard2014latent} under stochastic distribution assumption.


\textbf{Rank-$d$ Setting:} The work of \citet{kveton2017stochastic} can be viewed as a generalization of rank-$1$ bandits of \citet{katariya2016stochastic} to a higher rank of $d$. However, this work proposes a phase-based algorithm that calculates the square of the determinant of a $d\times d$ sub-matrix to eliminate sub-optimal rows and columns at the end of phases which is impractical for very large non-negative low-rank matrices. The theoretical guarantees hold for only stochastic distributions. Some other approaches involving non-negative matrix factorization \citet{sen2016contextual} or tensor based methods \citep{gopalan2016low} to reconstruct the matrix have also been proposed. These works require strong assumptions on the structure of the matrix such as all the matrices satisfy a weak statistical Restricted Isometric Property (RIP) or calculate third order tensors as in \citet{anandkumar2014tensor}. On the contrary, our simple and statistically efficient algorithm is easily generalizable to rank-$d$ and do not require any sort of costly matrix inversion or reconstruction operations or even row or column eliminations and hence are much easier to implement. 

%\textbf{â€¢} Our approach is based on two key insights. \todoan{I wouldn't call these key insights. These are points for how we are different from earlier papers.}First, the earlier methods (like Upper Confidence Bound (UCB) algorithms, NMF-Bandits \citep{sen2016contextual}) are explicitly modeled on the stochastic i.i.d assumption on feedback and cannot perform well in non-stochastic settings. Moreover, their theoretical guarantees will also fail in non-stochastic setting. Hence, we need algorithms that can work on more generalized non-stochastic probability distribution settings. Secondly, we can formulate simple and computationally efficient algorithms that learn the best set of columns and best set of rows jointly with two separate non-stochastic bandit algorithm operating on rows and columns individually. These do not require any sort of costly matrix inversion or reconstruction operations or even row or column eliminations and hence are much easier to implement. 


%Previous works that have studied this setting have either proposed highly conservative algorithms or restricted themselves to a stricter set of assumptions. While \citet{katariya2016stochastic} was proposed for a rank $1$ bandit model with the assumption that the underlying distributions are stochastic, \citet{katariya2017bernoulli} was proposed for the special case when the underlying distributions are Bernoulli. Both these works used different variations of the phase-based UCB-Improved \citep{auer2010ucb} algorithm to construct a confidence interval set over row-column pairs to identify and eliminate sub-optimal rows and columns. These naturally results in algorithms that explore conservatively (for the sake of row and column elimination) and cannot work beyond the stochastic distribution assumption. Finally, \citet{kveton2017stochastic} can be viewed as a generalization of rank-$1$ bandits of \citet{katariya2016stochastic} to a higher rank of $d$. However, this work proposes a phase-based algorithm that calculates the square of the determinant of a $d\times d$ sub-matrix to eliminate sub-optimal rows and columns at the end of phases which is impractical for very large non-negative low-rank matrices. Some other approaches involving non-negative matrix factorization \citet{sen2016contextual} or tensor based methods \citep{gopalan2016low} to reconstruct the matrix have also been proposed. These works require strong assumptions on the structure of the matrix such as all the matrices satisfy a weak statistical Restricted Isometric Property (RIP) or calculate third order tensors as in \citet{anandkumar2014tensor}. A more simpler setting has also been studied in \citet{maillard2014latent}.

%\todob{The two paragraphs above and below should be moved to "Related Work''. They contain too many details that are not necessary to understand our design and contributions. Put "Related Work'' right before "Conclusions''. Also, the current comparison to prior work is a laundry list and completely inefficient. Better structure how we differ. Paragraph 1: Some people do only rank $1$. We do rank $d$. Paragraph 2: Most papers do stochastic. We do adversarial (with restrictions). Paragraph 3: Our main selling point is a simple algorithm that can be easily generalized beyond rank $1$. Focus on this difference.}

%are detailed in Section \ref{sec:related work}
 %\citet{sen2016contextual} is an online matrix completion algorithm which is an $\epsilon$-greedy algorithm that tries to reconstruct the matrix $M$ through non-negative matrix factorization. Note, that this approach requires that all the matrices satisfy a weak statistical Restricted Isometric Property, which is not always feasible in real life applications. Another approach is that of \citet{gopalan2016low} where the authors come up with an algorithm which uses the Robust Tensor Power (RTP) method of 
%\citet{anandkumar2014tensor} to reconstruct the matrix $M$, and then use the OFUL procedure of \citet{abbasi2011improved} to behave greedily over the reconstructed matrix. 
%But the RTP is a costly operation because the learner needs to construct a matrix of order $L\times L$ and $L\times L \times L$ to calculate the second and third order tensors for the reconstruction.  A more simpler setting has also been studied in \citet{maillard2014latent}

%Our approach is based on two key insights. \todoan{I wouldn't call these key insights. These are points for how we are different from earlier papers.}First, the earlier methods (like Upper Confidence Bound (UCB) algorithms, NMF-Bandits \citep{sen2016contextual}) are explicitly modeled on the stochastic i.i.d assumption on feedback and cannot perform well in non-stochastic settings. Moreover, their theoretical guarantees will also fail in non-stochastic setting. Hence, we need algorithms that can work on more generalized non-stochastic probability distribution settings. Secondly, we can formulate simple and computationally efficient algorithms that learn the best set of columns and best set of rows jointly with two separate non-stochastic bandit algorithm operating on rows and columns individually. These do not require any sort of costly matrix inversion or reconstruction operations or even row or column eliminations and hence are much easier to implement. 