%!TEX root = paper.tex

In this work, we study the problem of learning personalized ranked lists of diverse items for multiple users, from sequential observations of user preferences. We are interested in utilizing latent similarities among users and items to learn these lists much faster than learning a separate ranked list for each user. The key structure in our problem is that the user-item preference matrix is low rank, which is a standard assumption in recommender systems \citep{koren2009matrix,ricci2011liorrokach}. The learning agent has access to noisy observations of the user-item matrix. It does not have access to either user or item latent factors.

We formalize our learning problem as the following online learning problem. At round $t$, a random user $i_t$ from a pool of $K$ users arrives to the recommender system. The learning agent observes the identity of the user $i_t$, recommends a list of $d$ diverse items $J_t$ from a pool of $L$ items as a response, and observes the preferences of user $i_t$ for \textit{all} recommended items $J_t$. The user-item preference matrix is low-rank at each round $t$, can vary substantially over time, and does not have to be stochastic. The reward of the recommended list is high when highly preferred items of the user are recommended at higher positions. The goal of our learning agent is to compete with the most rewarding diverse list for each user in hindsight.

Our learning model is motivated by a real-world scenario, where the learning agent suggests movies to users and each movie belongs to different movie genres. The agent typically does not observe instantaneous preferences of the user, and therefore suggests multiple movies that may be of interest to the user under different circumstances. A similar model has also been studied in \citet{carbonell1998use} where the goal is to suggest a diversified list to each incoming user that combines relevance to the query as well as novelty. The authors suggest an approach where each item in the list is relevant to the query but also has \textit{"marginal relevance"} or less similarity with previously selected documents and this improves the quality of recommendation.


%\todob{This is actually the main motivation for diversity. We need to add proper references and discuss this in detail.}
%
%\todosb{I wrote about main motivation behind diversity }

We make three major contributions. First, we formulate our online learning problem as a latent ranked bandit on low-rank matrices. We identify a family of non-negative low-rank matrices where our problem can be solved statistically efficiently, without estimating the latent factors of the user-item preference matrix. The key structure of our matrix is that the set of optimal items of all users is small and can be learned jointly for all users. Given these items, the problem of learning the optimal order for each user can be solved in the full-information setting and thus is easy. Second, we propose a computationally-efficient algorithm that implements this idea, which we call latent ranker algorithm ($\latentranker$). The algorithm has two components, column learning and row ranking, which learn the set of optimal items of all users and then sort them, respectively. The column learning algorithm is similar to ranked bandits. In particular, we learn the $k$-th most diverse item using a multi-armed bandit, whose rewards are conditioned on the rewards of $k - 1$ previously chosen items. The row learning problem is solved separately for each user. Because it is in the full-information setting, as we observe the individual rewards of all recommended items,  we solve it using the weighted majority algorithm. Third, we evaluate $\latentranker$ empirically on several synthetic and real-world problems. Perhaps surprisingly, $\latentranker$ performs well even when our modeling assumptions are violated.
%analyze $\latentranker$ and up to problem-specific factors, we prove a $O\left(d \sqrt{L n} + K \log n + K d \log d\right)$ upper bound on its $n$-step regret we . The regret of a naive solution is $O(\sqrt{K L n})$, and is much worse than that of $\latentranker$ when all of $K$, $L$, and $n$ are large. Finally, 

The paper is organized as follows. We introduce necessary background to understand our work in \cref{sec:background} and define our online learning problem in \cref{sec:setting}. We propose our algorithm in \cref{sec:algorithm} and analyze its regret in \cref{sec:analysis}. In \cref{sec:experiments}, we evaluate the algorithm empirically. In \cref{sec:related work}, we survey related work. We conclude in \cref{sec:conclusions}. 

%The detailed proof of our regret bound is presented in \cref{sec:proof}.
