%Extending the discussion in the previous section, we propose a Generalized Latent Bandit model where each $v_{a,b}$ can be considered as a mixture of several $\lbrace v_{a,c}\rbrace_{c\in\C}$. This is a harder problem than Latent Bandit model because now the rows in each cluster are not exactly identical but the index of the optimal arm (column) is similar in each row.

Throughout the paper, we denote $[n] = \lbrace 1,2,\ldots, n\rbrace$.
An instance of the \emph{Low-Rank Bandit} problem is a matrix
$R\in [0,1]^{K\times L}$ representing the expected click-through rates (CTRs) for each
user $k\in [K]$ on each item $l\in [L]$.
If, $J\subset [L]$ is a subset of columns, we denote $R(:,J)\in [0,1]^{K,|J|}$ the corresponding
submatrix containing the $|J|$ columns of $R$.

We assume that there exists a latent structure, i.e that $R=UV^T$ where the rows of
$U$ and $V$ contain the hidden users' and item's features.
It is important to notice that none of those features are observable, meaning that
we cannot build on a linear bandit model, and in particular
our problem cannot be seen as a \emph{clustering of bandits} problem \cite{gentile2014online}.
%% that last sentence is for our friend Suai ;)
However, the rank of the CTR matrix is  assumed to be low, that is $d << \min\lbrace L,K\rbrace$.
This is the key assumption of our model.
It implies, by definition, the following property.

\begin{observation}
	Let $M\in \mathbb{R}^{K\times L}$ be a rank-$d$ matrix. Then,
	\begin{itemize}
		\item There exists a basis $J^*$ of $d$ column such that all the $L$ columns' latent features
		are linear combinations of the vectors in $J^*$;
		\item There exists a basis $I^*$ of $d$ users such that all the $K$ users' latent features
		are linear combinations of the vectors in $I^*$.
	\end{itemize}
	Without loss of generality, the above mentioned bases can be chosen of maximal volume such that
	the corresponding transformation matrix is the least singular possible.
\end{observation}
\begin{proof}
	The existence of the basis on both dimensions comes directly by definition of the
	low rank assumption. The choice of the spanning vectors is arbitrary and maximising the
	volume means choosing vectors with larger norm and hence potentially larger payoff.
\end{proof}

\todocla{Here state the result on the existence of a best set of $d$ items, I'm not sure how to state it. It is not an ``assumption'' though, it is a Lemma or a Fact but not an assumption. It is a consequence of the low rank assumption :)}


The interaction at round $t\geq 1$ of the learner with the online recommender system characterized by $R$
goes as follows:
\begin{itemize}
	\item a user $i_t\in [K]$ shows up -- it corresponds to the index of a row of the matrix. It can be seen as an unobserved context generated by the environment;
	\item the learner chooses a set $J_t\subset [L]$ such that $|J_t|=d$ to be sequentially presented to the user;
	\item the user browses those $d$ options and send an individual feedback for each of them (semi-bandit setting): $\forall j\in J_t$, the learner observes $Y_{t,j}= R(i_t,j) + \eta_{t,j}$ where $(\eta_{t,j})_{t,j\geq 0}$ is a seqence of i.i.d centered random variables. \todocla{fix your noise model here. Bernoulli ?? }
\end{itemize}

For each user $i\in [K]$, there exists one unique best item $j^*(i)\in [L]$
\todocla{Define the best item, define the expected regret}

The objective of the learning agent is to minimize the expected cumulative regret up to horizon $n$. We define the cumulative regret, denoted by $\mathcal{R}_n$ as,

\begin{align*}
%\mathcal{R}_n = \mathds{E}\sum_{t=1}^{n}\bigg\lbrace \sum_{j\in J_t} \left( r_{t}\left(i_{t}, j^*_{t,z} \right) - r_{t}\left( i_{t}, j_{t,z}\right)\bigg)\bigg\rbrace
\end{align*}


% 	We define $[n] = \lbrace 1,2,\ldots, n\rbrace$ and for any two sets $A$ and $B$, $A^B$ denotes the set of all vectors who take values from $A$ and are indexed by $B$. Let, $M\in [0,1]^{K\times L}$ denote any matrix, then $M(I,:)$ denote any submatrix of $k$ rows such that $I\in[K]^k$ and similarly $R(:,J)$ denote any submatrix of $j$ columns such that $J\in[L]^{j}$.
%
% 	Let $\bar{M}$ be reward matrix of  dimension $K\times L$ where $K$ is the number of user or rows and $L$ is the number of arms or columns. Also, let us assume that this matrix  $\bar{M}$ has a low rank structure of rank $d << \min\lbrace L,K\rbrace$. Let $U$ and $V$ denote the latent matrices for the users and items, which are not visible to the learner such that,
% \begin{align*}
% 	\bar{M} = UV^{\intercal} \textbf{ \hspace*{4mm}   s.t.   \hspace*{4mm}} U\in [ \mathbb{R}^+ ]^{K\times d} \textbf{, } V\in  [0,1]^{L\times d}
% \end{align*}
%
% 	Furthermore, we put a constraint on $V$ such that, $\forall j\in [L]$, $ \norm{V(j,:)}_1 \leq 1$.
%
%
% \begin{assumption}
% \label{assm:1}
% We assume that there exists $d$-column base factors, denoted by $V(J^*,:)$, such that all rows of $V$ can be written as a convex combination of $V(J^*,:)$ and the zero vector and $J^* = [d]$. We denote the column factors by $V^* = V(J*,:)$. Therefore, for any $i\in [L]$, it can be represented by
% \begin{align*}
% V(i,:) = a_i V(J^*,:) ,
% \end{align*}
% where $\exists a_i\in [0,1]^{d}$ and $ \norm{a_i}_1 \leq 1$.
% \end{assumption}
%
%
% \begin{assumption}
% \label{assm:d-items}
% For each user $i_t$ revealed by the nature at round $t$, the learner is allowed to select atmost $d$-items, where $d$ is the rank of the matrix $\bar{R}$.
% \end{assumption}
%
% The above assumption \ref{assm:d-items} can be conceptualized in this real-world scenario where the learner has  to suggest movies to users and each movie belongs to a different genre (say thriller, romance, comedy, etc). So, the learner can suggest $d$ movies belonging to different genres to each user, and the user can click one, or all, or none of the recommended movies.
%
%
% The main goal of the learning agent is to minimize the cumulative regret until the end of horizon $n$. We define the cumulative regret, denoted by $\mathcal{R}_n$ as,
%
% \begin{align*}
% \mathcal{R}_n = \sum_{t=1}^{n}\bigg\lbrace \sum_{z=1}^{d} \bigg( r_{t}\left(i_{t}, j^*_{t,z} \right) - r_{t}\left( i_{t}, j_{t,z}\right)\bigg)\bigg\rbrace
% \end{align*}
%
% where, $j^*_{t,z} = \argmax_{j\in [L]}\lbrace \bar{M}(i_t,j)\rbrace$ and $j_{t,z}$ be the suggestion of the learner for the $i_t$ -th user for  $z=1,2,\ldots, d$. Note that $r_{t}\left(i_t, j^*_{z,t} \right)\sim \D(\bar{M}\left(i_t, j^*_{z,t} \right))$ and $r_{t}\left(i_t, j_{z,t} \right)\sim \D(\bar{M}\left(i_t, j_{z,t} \right))$. Taking expectation over both sides, we can show that,
%
% \begin{align*}
% \E[\mathcal{R}_n] = \E\left [ \sum_{t=1}^{n}\bigg\lbrace\sum_{z=1}^{d} \bigg( r_{z,t}\left(i_t, j^*_{z,t} \right) - r_{z, t}\left( i_t, j_{z, t}\right)\bigg)\bigg\rbrace\right] = \E\left [ \sum_{t=1}^{T} \sum_{z=1}^{d} \bigg( N_{i_t,j_{z,t}}\bigg) \right ]\Delta_{i_t,j_{z,t}}
% \end{align*}
%
% where, $\Delta_{i_t,j_{z,t}} = \bar{M}(i_t,j^*_{z,t}) - \bar{M}(i_t,j_{z,t})$ and $N_{i_t,j_{z,t}}$ is the number of times the learner has observed the $j_{z,t}$-th item for the $i_t$-th user for $z=1,2,\ldots, d$. Let, $\Delta = \min_{i\in[K],j\in[L]}\lbrace \Delta_{i,j}\rbrace$ be the minimum gap over all the user, item pair in $\bar{M}$.
